{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock value prediction from Open, High, Low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Import module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt2\n",
    "import pandas as pd\n",
    "from pandas import datetime\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "import math, time\n",
    "import itertools\n",
    "from sklearn import preprocessing\n",
    "import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import load_model\n",
    "import keras\n",
    "import pandas_datareader.data as web\n",
    "import h5py\n",
    "import os\n",
    "import keras.backend as K\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "\n",
    "from collections import deque\n",
    "from io import StringIO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_name = '^GSPC'\n",
    "seq_len = 22\n",
    "d = 0.2\n",
    "shape = [4, seq_len, 1] # feature, window, output\n",
    "neurons = [128, 128, 32, 1]\n",
    "epochs = 200\n",
    "\n",
    "neurons_layer = 1\n",
    "optimizer = 'Adam'\n",
    "loss = 'mse'\n",
    "action = 'train'\n",
    "# loss = 'risk_estimation'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1. Download data and normalize it\n",
    "Data since date 'start' to today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_df(df):\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    for feature in df.columns:\n",
    "        df[feature] = min_max_scaler.fit_transform(df[feature].values.reshape(-1,1))\n",
    "    return df\n",
    "\n",
    "def denormalize(normalized_value):\n",
    "#     for feature in df.columns:\n",
    "#         df[feature] = min_max_scaler.fit_transform(df[feature].values.reshape(-1,1))\n",
    "    \n",
    "#     min_max_scaler.inverse_transform(normalized_value)\n",
    "    \n",
    "#     normalized_value = normalized_value.reshape(-1,1)\n",
    "    \n",
    "#     #return df.shape, p.shape\n",
    "#     min_max_scaler = preprocessing.MinMaxScaler()\n",
    "#     a = min_max_scaler.fit_transform(df)\n",
    "#     new = min_max_scaler.inverse_transform(normalized_value)\n",
    "    return normalized_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dateutil.parser\n",
    "def get_last_date(stock_name):\n",
    "    data_path = 'data/' + stock_name\n",
    "    if os.path.exists(data_path):\n",
    "        with open(data_path, 'r') as f:\n",
    "            last_day_string = deque(f, 1).pop().split(\",\")[0]\n",
    "            start = datetime.datetime.strptime(last_day_string, '%Y-%m-%d') + datetime.timedelta(days=1)\n",
    "            start = start.date()\n",
    "            return start\n",
    "    else:\n",
    "        return datetime.datetime(2013, 1, 1).date()\n",
    "\n",
    "def update_stock_data(stock_name):\n",
    "    start = get_last_date(stock_name)\n",
    "    end = datetime.date.today()\n",
    "    data_path = 'data/' + stock_name\n",
    "    \n",
    "    if os.path.exists(data_path):\n",
    "        with open(data_path, 'r') as f:\n",
    "            last_day_string = deque(f, 1).pop().split(\",\")[0]\n",
    "            start = datetime.datetime.strptime(last_day_string, '%Y-%m-%d')\n",
    "            start = start.date()\n",
    "\n",
    "    if start > datetime.date.today():\n",
    "        print ('stock data already latest')\n",
    "        return\n",
    "    else:\n",
    "        try:\n",
    "            df = web.DataReader(stock_name, \"yahoo\", start)\n",
    "        except:\n",
    "            print('retrying data from yahoo...')\n",
    "            update_stock_data(stock_name)\n",
    "            return\n",
    "        \n",
    "        if os.path.exists(data_path):\n",
    "            with open(data_path, 'a') as f:\n",
    "#                 print('df = ',df, 'start = ', start)\n",
    "                mask = df.index > str(start)\n",
    "                df = df[mask]\n",
    "                df.to_csv(f, header=False)\n",
    "                print ('stock data appended')\n",
    "        else:\n",
    "            df.to_csv(data_path)\n",
    "            print ('stock data create')\n",
    "        \n",
    "                \n",
    "def get_stock_data(stock_name, normalize=True, use_exist_data=True):\n",
    "    data_path = 'data/' + stock_name\n",
    "    df = pd.read_csv(os.path.join(data_path))\n",
    "    \n",
    "    df.drop(['Volume', 'Close', 'Date'], 1, inplace=True)\n",
    "    \n",
    "    if normalize:\n",
    "#         df = df\n",
    "        df = normalize_df(df)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock data appended\n"
     ]
    }
   ],
   "source": [
    "update_stock_data(stock_name)\n",
    "df = get_stock_data(stock_name, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Plot out the Normalized Adjusted close price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stock(stock_name):\n",
    "    df = get_stock_data(stock_name, normalize=False)\n",
    "    print(df.head())\n",
    "    plt.plot(df['Adj Close'], color='red', label='Adj Close')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Open         High          Low    Adj Close\n",
      "0  1402.430054  1426.739990  1398.109985  1426.189941\n",
      "1  1426.189941  1462.430054  1426.189941  1462.420044\n",
      "2  1462.420044  1465.469971  1455.530029  1459.369995\n",
      "3  1459.369995  1467.939941  1458.989990  1466.469971\n",
      "4  1466.469971  1466.469971  1456.619995  1461.890015\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a17d5e588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_stock(stock_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Set last day Adjusted Close as y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(stock, seq_len):\n",
    "    amount_of_features = len(stock.columns)\n",
    "    data = stock.as_matrix() \n",
    "    sequence_length = seq_len + 1 # index starting from 0\n",
    "    result = []\n",
    "    \n",
    "    for index in range(len(data) - sequence_length): # maxmimum date = lastest date - sequence length\n",
    "        result.append(data[index: index + sequence_length]) # index : index + 22days\n",
    "    \n",
    "    result = np.array(result)\n",
    "    row = round(0.9 * result.shape[0]) # 90% split\n",
    "    \n",
    "    train = result[:int(row), :] # 90% date\n",
    "    X_train = train[:, :-1] # all data until day m\n",
    "    y_train = train[:, -1][:,-1] # day m + 1 adjusted close price\n",
    "    \n",
    "    X_test = result[int(row):, :-1]\n",
    "    y_test = result[int(row):, -1][:,-1] \n",
    "\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], amount_of_features))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], amount_of_features))  \n",
    "\n",
    "    return [X_train, y_train, X_test, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = load_data(df, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1130, 22, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[0], X_train.shape[1], X_train.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1130"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Buidling neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model2(layers, neurons, d):\n",
    "    model = Sequential()\n",
    "    \n",
    "    for i in range(neurons_layer):\n",
    "        model.add(LSTM(neurons[0], input_shape=(layers[1], layers[0]), return_sequences=True))\n",
    "        model.add(Dropout(d))\n",
    "\n",
    "    model.add(LSTM(neurons[1], input_shape=(layers[1], layers[0]), return_sequences=False))\n",
    "    model.add(Dropout(d))\n",
    "        \n",
    "    model.add(Dense(neurons[2],kernel_initializer=\"uniform\",activation='relu'))        \n",
    "    model.add(Dense(neurons[3],kernel_initializer=\"uniform\",activation='linear'))\n",
    "    # model = load_model('my_LSTM_stock_model1000.h5')\n",
    "    # adam = keras.optimizers.Adam(decay=0.2)\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Model Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = build_model2(shape, neurons, d)\n",
    "# # layers = [4, 22, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(\n",
    "#     X_train,\n",
    "#     y_train,\n",
    "#     batch_size=512,\n",
    "#     epochs=epochs,\n",
    "#     validation_split=0.1,\n",
    "#     verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Result on training set and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def model_score(model, X_train, y_train, X_test, y_test):\n",
    "#     trainScore = model.evaluate(X_train, y_train, verbose=0)\n",
    "#     print('Train Score: %.5f MSE (%.2f RMSE)' % (trainScore[0], math.sqrt(trainScore[0])))\n",
    "\n",
    "#     testScore = model.evaluate(X_test, y_test, verbose=0)\n",
    "#     print('Test Score: %.5f MSE (%.2f RMSE)' % (testScore[0], math.sqrt(testScore[0])))\n",
    "#     return trainScore[0], testScore[0]\n",
    "\n",
    "# model_score(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Prediction vs Real results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage_difference(model, X_test, y_test):\n",
    "    percentage_diff=[]\n",
    "\n",
    "    p = model.predict(X_test)\n",
    "    for u in range(len(y_test)): # for each data index in test data\n",
    "        pr = p[u][0] # pr = prediction on day u\n",
    "\n",
    "        percentage_diff.append((pr-y_test[u]/pr)*100)\n",
    "    return percentage_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = percentage_difference(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Plot out prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_value(stock_name, predict, actual, dir_name, file_name):\n",
    "    if not os.path.isdir(dir_name):\n",
    "        os.makedirs(dir_name)\n",
    "    \n",
    "    fig_path = dir_name + '/' + file_name\n",
    "    \n",
    "    plt2.plot(predict, color='red', label='Prediction')\n",
    "    plt2.plot(actual,color='blue', label='Actual')\n",
    "    plt2.legend(loc='best')\n",
    "    plt2.title('The test result for {}'.format(stock_name))\n",
    "    plt2.xlabel('Days')\n",
    "    plt2.ylabel('Adjusted Close')\n",
    "    plt2.savefig(fig_path, format='png', bbox_inches='tight', transparent=True)\n",
    "    plt2.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(stock_name, normalized_value_p, normalized_value_y_test, epochs, date):\n",
    "    newp = denormalize(normalized_value_p)\n",
    "    newy_test = denormalize(normalized_value_y_test)    \n",
    "    subdir = '/' +  date.strftime(\"%Y%m%d%H%M%S\") + '/' + 'price' \n",
    "    dir_name = str(neurons_layer) + 'layer_' + optimizer + stock_name + subdir \n",
    "    figname = stock_name + '_epochs' + str(epochs) + '.png'\n",
    "    plot_value(stock_name, newp, newy_test, dir_name, figname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 損益計算\n",
    "def plot_value_change_and_revenue(stock_name, normalized_value_p, normalized_value_y_test, epochs, date):\n",
    "    newp = denormalize(normalized_value_p)\n",
    "    newy_test = denormalize(normalized_value_y_test)\n",
    "    \n",
    "    volumn_change_p = []\n",
    "    volumn_change_y_test = []\n",
    "    revenue_p = []\n",
    "    revenue_y_test = []\n",
    "    total_revenue_p = []\n",
    "    total_revenue_y_test = []\n",
    "    for i in range(len(newp)):\n",
    "        if i == 0:\n",
    "            volumn_change_p.append(0)\n",
    "            volumn_change_y_test.append(0)\n",
    "            \n",
    "            revenue_p.append(0)\n",
    "            revenue_y_test.append(0)\n",
    "            \n",
    "            total_revenue_p.append(0)\n",
    "            total_revenue_y_test.append(0)\n",
    "        else:\n",
    "            volumn_change_p.append(newp[i] - newp[i-1])\n",
    "            volumn_change_y_test.append(newy_test[i] - newy_test[i-1])\n",
    "\n",
    "            if volumn_change_p[i] > volumn_change_p[i-1]:\n",
    "                revenue_p.append(volumn_change_y_test[i] - volumn_change_y_test[i - 1])\n",
    "            else:\n",
    "                revenue_p.append(volumn_change_y_test[i - 1] - volumn_change_y_test[i])\n",
    "            \n",
    "            test_earn = abs(volumn_change_y_test[i] - volumn_change_y_test[i - 1])\n",
    "            revenue_y_test.append(test_earn)\n",
    "            \n",
    "            total_revenue_p.append(total_revenue_p[i - 1] + revenue_p[i])\n",
    "            total_revenue_y_test.append(total_revenue_y_test[i - 1] + revenue_y_test[i])\n",
    "            \n",
    "    figname = stock_name + '_epochs' + str(epochs) + '.png'\n",
    "    \n",
    "    dir_prefix = str(neurons_layer) + 'layer_' + optimizer + stock_name\n",
    "\n",
    "    subdir = '/' +  date.strftime(\"%Y%m%d%H%M%S\") + '/' + 'value_change'     \n",
    "    dir_name = dir_prefix + subdir\n",
    "    plot_value(stock_name, volumn_change_p, volumn_change_y_test, dir_name, figname)\n",
    "    \n",
    "    subdir = '/' +  date.strftime(\"%Y%m%d%H%M%S\") + '/' + 'revenue' \n",
    "    dir_name = dir_prefix + subdir\n",
    "    plot_value(stock_name, revenue_p, revenue_y_test, dir_name, figname)\n",
    "    \n",
    "    subdir = '/' +  date.strftime(\"%Y%m%d%H%M%S\") + '/' + 'revenue_sum' \n",
    "    dir_name = dir_prefix + subdir\n",
    "    plot_value(stock_name, total_revenue_p, total_revenue_y_test, dir_name, figname)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Save for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 22, 128)           68096     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 22, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 203,841\n",
      "Trainable params: 203,841\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1017 samples, validate on 113 samples\n",
      "Epoch 1/25\n",
      "1017/1017 [==============================] - 1s 1ms/step - loss: 0.1566 - acc: 0.0000e+00 - val_loss: 0.4570 - val_acc: 0.0000e+00\n",
      "Epoch 2/25\n",
      "1017/1017 [==============================] - 1s 817us/step - loss: 0.1487 - acc: 0.0000e+00 - val_loss: 0.4252 - val_acc: 0.0000e+00\n",
      "Epoch 3/25\n",
      "1017/1017 [==============================] - 1s 783us/step - loss: 0.1348 - acc: 0.0000e+00 - val_loss: 0.3588 - val_acc: 0.0000e+00\n",
      "Epoch 4/25\n",
      "1017/1017 [==============================] - 1s 759us/step - loss: 0.1051 - acc: 0.0000e+00 - val_loss: 0.2349 - val_acc: 0.0000e+00\n",
      "Epoch 5/25\n",
      "1017/1017 [==============================] - 1s 817us/step - loss: 0.0506 - acc: 0.0000e+00 - val_loss: 0.0994 - val_acc: 0.0000e+00\n",
      "Epoch 6/25\n",
      "1017/1017 [==============================] - 1s 723us/step - loss: 0.0108 - acc: 0.0000e+00 - val_loss: 0.0243 - val_acc: 0.0000e+00\n",
      "Epoch 7/25\n",
      "1017/1017 [==============================] - 1s 776us/step - loss: 0.0291 - acc: 0.0000e+00 - val_loss: 0.0222 - val_acc: 0.0000e+00\n",
      "Epoch 8/25\n",
      "1017/1017 [==============================] - 1s 768us/step - loss: 0.0284 - acc: 0.0000e+00 - val_loss: 0.0486 - val_acc: 0.0000e+00\n",
      "Epoch 9/25\n",
      "1017/1017 [==============================] - 1s 765us/step - loss: 0.0166 - acc: 0.0000e+00 - val_loss: 0.0862 - val_acc: 0.0000e+00\n",
      "Epoch 10/25\n",
      "1017/1017 [==============================] - 1s 741us/step - loss: 0.0141 - acc: 0.0000e+00 - val_loss: 0.1168 - val_acc: 0.0000e+00\n",
      "Epoch 11/25\n",
      "1017/1017 [==============================] - 1s 743us/step - loss: 0.0165 - acc: 0.0000e+00 - val_loss: 0.1302 - val_acc: 0.0000e+00\n",
      "Epoch 12/25\n",
      "1017/1017 [==============================] - 1s 878us/step - loss: 0.0180 - acc: 0.0000e+00 - val_loss: 0.1286 - val_acc: 0.0000e+00\n",
      "Epoch 13/25\n",
      "1017/1017 [==============================] - 1s 857us/step - loss: 0.0166 - acc: 0.0000e+00 - val_loss: 0.1153 - val_acc: 0.0000e+00\n",
      "Epoch 14/25\n",
      "1017/1017 [==============================] - 1s 836us/step - loss: 0.0140 - acc: 0.0000e+00 - val_loss: 0.0952 - val_acc: 0.0000e+00\n",
      "Epoch 15/25\n",
      "1017/1017 [==============================] - 1s 951us/step - loss: 0.0109 - acc: 0.0000e+00 - val_loss: 0.0741 - val_acc: 0.0000e+00\n",
      "Epoch 16/25\n",
      "1017/1017 [==============================] - 1s 899us/step - loss: 0.0083 - acc: 0.0000e+00 - val_loss: 0.0557 - val_acc: 0.0000e+00\n",
      "Epoch 17/25\n",
      "1017/1017 [==============================] - 1s 881us/step - loss: 0.0068 - acc: 0.0000e+00 - val_loss: 0.0439 - val_acc: 0.0000e+00\n",
      "Epoch 18/25\n",
      "1017/1017 [==============================] - 1s 750us/step - loss: 0.0048 - acc: 0.0000e+00 - val_loss: 0.0391 - val_acc: 0.0000e+00\n",
      "Epoch 19/25\n",
      "1017/1017 [==============================] - 1s 735us/step - loss: 0.0029 - acc: 0.0000e+00 - val_loss: 0.0374 - val_acc: 0.0000e+00\n",
      "Epoch 20/25\n",
      "1017/1017 [==============================] - 1s 738us/step - loss: 0.0030 - acc: 0.0000e+00 - val_loss: 0.0282 - val_acc: 0.0000e+00\n",
      "Epoch 21/25\n",
      "1017/1017 [==============================] - 1s 732us/step - loss: 0.0021 - acc: 0.0000e+00 - val_loss: 0.0142 - val_acc: 0.0000e+00\n",
      "Epoch 22/25\n",
      "1017/1017 [==============================] - 1s 810us/step - loss: 0.0019 - acc: 0.0000e+00 - val_loss: 0.0083 - val_acc: 0.0000e+00\n",
      "Epoch 23/25\n",
      "1017/1017 [==============================] - 1s 882us/step - loss: 0.0019 - acc: 0.0000e+00 - val_loss: 0.0096 - val_acc: 0.0000e+00\n",
      "Epoch 24/25\n",
      "1017/1017 [==============================] - 1s 906us/step - loss: 0.0020 - acc: 0.0000e+00 - val_loss: 0.0092 - val_acc: 0.0000e+00\n",
      "Epoch 25/25\n",
      "1017/1017 [==============================] - 1s 793us/step - loss: 0.0019 - acc: 0.0000e+00 - val_loss: 0.0040 - val_acc: 0.0000e+00\n",
      "Train on 1017 samples, validate on 113 samples\n",
      "Epoch 1/25\n",
      "1017/1017 [==============================] - 1s 731us/step - loss: 0.0020 - acc: 0.0000e+00 - val_loss: 0.0039 - val_acc: 0.0000e+00\n",
      "Epoch 2/25\n",
      "1017/1017 [==============================] - 1s 739us/step - loss: 0.0015 - acc: 0.0000e+00 - val_loss: 0.0069 - val_acc: 0.0000e+00\n",
      "Epoch 3/25\n",
      "1017/1017 [==============================] - 1s 755us/step - loss: 0.0017 - acc: 0.0000e+00 - val_loss: 0.0059 - val_acc: 0.0000e+00\n",
      "Epoch 4/25\n",
      "1017/1017 [==============================] - 1s 838us/step - loss: 0.0014 - acc: 0.0000e+00 - val_loss: 0.0042 - val_acc: 0.0000e+00\n",
      "Epoch 5/25\n",
      "1017/1017 [==============================] - 1s 720us/step - loss: 0.0015 - acc: 0.0000e+00 - val_loss: 0.0059 - val_acc: 0.0000e+00\n",
      "Epoch 6/25\n",
      "1017/1017 [==============================] - 1s 713us/step - loss: 0.0012 - acc: 0.0000e+00 - val_loss: 0.0088 - val_acc: 0.0000e+00\n",
      "Epoch 7/25\n",
      "1017/1017 [==============================] - 1s 857us/step - loss: 0.0013 - acc: 0.0000e+00 - val_loss: 0.0077 - val_acc: 0.0000e+00\n",
      "Epoch 8/25\n",
      "1017/1017 [==============================] - 1s 722us/step - loss: 0.0013 - acc: 0.0000e+00 - val_loss: 0.0057 - val_acc: 0.0000e+00\n",
      "Epoch 9/25\n",
      "1017/1017 [==============================] - 1s 814us/step - loss: 0.0013 - acc: 0.0000e+00 - val_loss: 0.0071 - val_acc: 0.0000e+00\n",
      "Epoch 10/25\n",
      "1017/1017 [==============================] - 1s 750us/step - loss: 0.0013 - acc: 0.0000e+00 - val_loss: 0.0092 - val_acc: 0.0000e+00\n",
      "Epoch 11/25\n",
      "1017/1017 [==============================] - 1s 739us/step - loss: 0.0013 - acc: 0.0000e+00 - val_loss: 0.0077 - val_acc: 0.0000e+00\n",
      "Epoch 12/25\n",
      "1017/1017 [==============================] - 1s 754us/step - loss: 0.0013 - acc: 0.0000e+00 - val_loss: 0.0054 - val_acc: 0.0000e+00\n",
      "Epoch 13/25\n",
      "1017/1017 [==============================] - 1s 730us/step - loss: 0.0012 - acc: 0.0000e+00 - val_loss: 0.0061 - val_acc: 0.0000e+00\n",
      "Epoch 14/25\n",
      "1017/1017 [==============================] - 1s 774us/step - loss: 0.0012 - acc: 0.0000e+00 - val_loss: 0.0070 - val_acc: 0.0000e+00\n",
      "Epoch 15/25\n",
      "1017/1017 [==============================] - 1s 752us/step - loss: 0.0012 - acc: 0.0000e+00 - val_loss: 0.0055 - val_acc: 0.0000e+00\n",
      "Epoch 16/25\n",
      "1017/1017 [==============================] - 1s 833us/step - loss: 0.0012 - acc: 0.0000e+00 - val_loss: 0.0039 - val_acc: 0.0000e+00\n",
      "Epoch 17/25\n",
      "1017/1017 [==============================] - 1s 753us/step - loss: 0.0012 - acc: 0.0000e+00 - val_loss: 0.0046 - val_acc: 0.0000e+00\n",
      "Epoch 18/25\n",
      "1017/1017 [==============================] - 1s 750us/step - loss: 0.0012 - acc: 0.0000e+00 - val_loss: 0.0048 - val_acc: 0.0000e+00\n",
      "Epoch 19/25\n",
      "1017/1017 [==============================] - 1s 730us/step - loss: 0.0012 - acc: 0.0000e+00 - val_loss: 0.0034 - val_acc: 0.0000e+00\n",
      "Epoch 20/25\n",
      "1017/1017 [==============================] - 1s 777us/step - loss: 0.0012 - acc: 0.0000e+00 - val_loss: 0.0032 - val_acc: 0.0000e+00\n",
      "Epoch 21/25\n",
      "1017/1017 [==============================] - 1s 733us/step - loss: 0.0012 - acc: 0.0000e+00 - val_loss: 0.0035 - val_acc: 0.0000e+00\n",
      "Epoch 22/25\n",
      "1017/1017 [==============================] - 1s 733us/step - loss: 0.0012 - acc: 0.0000e+00 - val_loss: 0.0039 - val_acc: 0.0000e+00\n",
      "Epoch 23/25\n",
      "1017/1017 [==============================] - 1s 751us/step - loss: 0.0011 - acc: 0.0000e+00 - val_loss: 0.0030 - val_acc: 0.0000e+00\n",
      "Epoch 24/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1017/1017 [==============================] - 1s 768us/step - loss: 0.0011 - acc: 0.0000e+00 - val_loss: 0.0028 - val_acc: 0.0000e+00\n",
      "Epoch 25/25\n",
      "1017/1017 [==============================] - 1s 753us/step - loss: 0.0012 - acc: 0.0000e+00 - val_loss: 0.0034 - val_acc: 0.0000e+00\n",
      "Train on 1017 samples, validate on 113 samples\n",
      "Epoch 1/25\n",
      "1017/1017 [==============================] - 1s 713us/step - loss: 0.0011 - acc: 0.0000e+00 - val_loss: 0.0029 - val_acc: 0.0000e+00\n",
      "Epoch 2/25\n",
      "1017/1017 [==============================] - 1s 860us/step - loss: 0.0012 - acc: 0.0000e+00 - val_loss: 0.0028 - val_acc: 0.0000e+00\n",
      "Epoch 3/25\n",
      "1017/1017 [==============================] - 1s 819us/step - loss: 0.0011 - acc: 0.0000e+00 - val_loss: 0.0031 - val_acc: 0.0000e+00\n",
      "Epoch 4/25\n",
      "1017/1017 [==============================] - 1s 883us/step - loss: 0.0012 - acc: 0.0000e+00 - val_loss: 0.0025 - val_acc: 0.0000e+00\n",
      "Epoch 5/25\n",
      "1017/1017 [==============================] - 1s 794us/step - loss: 0.0011 - acc: 0.0000e+00 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 6/25\n",
      "1017/1017 [==============================] - 1s 891us/step - loss: 0.0011 - acc: 0.0000e+00 - val_loss: 0.0029 - val_acc: 0.0000e+00\n",
      "Epoch 7/25\n",
      "1017/1017 [==============================] - 1s 872us/step - loss: 0.0011 - acc: 0.0000e+00 - val_loss: 0.0026 - val_acc: 0.0000e+00\n",
      "Epoch 8/25\n",
      "1017/1017 [==============================] - 1s 827us/step - loss: 0.0012 - acc: 0.0000e+00 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 9/25\n",
      "1017/1017 [==============================] - 1s 834us/step - loss: 0.0011 - acc: 0.0000e+00 - val_loss: 0.0028 - val_acc: 0.0000e+00\n",
      "Epoch 10/25\n",
      "1017/1017 [==============================] - 1s 887us/step - loss: 0.0011 - acc: 0.0000e+00 - val_loss: 0.0024 - val_acc: 0.0000e+00\n",
      "Epoch 11/25\n",
      "1017/1017 [==============================] - 1s 826us/step - loss: 0.0011 - acc: 0.0000e+00 - val_loss: 0.0020 - val_acc: 0.0000e+00\n",
      "Epoch 12/25\n",
      "1017/1017 [==============================] - 1s 818us/step - loss: 0.0012 - acc: 0.0000e+00 - val_loss: 0.0022 - val_acc: 0.0000e+00\n",
      "Epoch 13/25\n",
      "1017/1017 [==============================] - 1s 809us/step - loss: 0.0012 - acc: 0.0000e+00 - val_loss: 0.0025 - val_acc: 0.0000e+00\n",
      "Epoch 14/25\n",
      "1017/1017 [==============================] - 1s 782us/step - loss: 0.0011 - acc: 0.0000e+00 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 15/25\n",
      "1017/1017 [==============================] - 1s 772us/step - loss: 0.0011 - acc: 0.0000e+00 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 16/25\n",
      "1017/1017 [==============================] - 1s 724us/step - loss: 0.0011 - acc: 0.0000e+00 - val_loss: 0.0023 - val_acc: 0.0000e+00\n",
      "Epoch 17/25\n",
      "1017/1017 [==============================] - 1s 709us/step - loss: 0.0011 - acc: 0.0000e+00 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 18/25\n",
      "1017/1017 [==============================] - 1s 713us/step - loss: 0.0011 - acc: 0.0000e+00 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 19/25\n",
      "1017/1017 [==============================] - 1s 721us/step - loss: 0.0011 - acc: 0.0000e+00 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 20/25\n",
      "1017/1017 [==============================] - 1s 712us/step - loss: 0.0010 - acc: 0.0000e+00 - val_loss: 0.0019 - val_acc: 0.0000e+00\n",
      "Epoch 21/25\n",
      "1017/1017 [==============================] - 1s 747us/step - loss: 0.0011 - acc: 0.0000e+00 - val_loss: 0.0015 - val_acc: 0.0000e+00\n",
      "Epoch 22/25\n",
      "1017/1017 [==============================] - 1s 745us/step - loss: 0.0011 - acc: 0.0000e+00 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 23/25\n",
      "1017/1017 [==============================] - 1s 730us/step - loss: 9.8402e-04 - acc: 0.0000e+00 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 24/25\n",
      "1017/1017 [==============================] - 1s 721us/step - loss: 0.0011 - acc: 0.0000e+00 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 25/25\n",
      "1017/1017 [==============================] - 1s 728us/step - loss: 0.0010 - acc: 0.0000e+00 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Train on 1017 samples, validate on 113 samples\n",
      "Epoch 1/25\n",
      "1017/1017 [==============================] - 1s 782us/step - loss: 0.0010 - acc: 0.0000e+00 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 2/25\n",
      "1017/1017 [==============================] - 1s 752us/step - loss: 0.0010 - acc: 0.0000e+00 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 3/25\n",
      "1017/1017 [==============================] - 1s 800us/step - loss: 0.0010 - acc: 0.0000e+00 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 4/25\n",
      "1017/1017 [==============================] - 1s 875us/step - loss: 0.0011 - acc: 0.0000e+00 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 5/25\n",
      "1017/1017 [==============================] - 1s 767us/step - loss: 0.0010 - acc: 0.0000e+00 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 6/25\n",
      "1017/1017 [==============================] - 1s 742us/step - loss: 0.0011 - acc: 0.0000e+00 - val_loss: 0.0018 - val_acc: 0.0000e+00\n",
      "Epoch 7/25\n",
      "1017/1017 [==============================] - 1s 763us/step - loss: 0.0010 - acc: 0.0000e+00 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 8/25\n",
      "1017/1017 [==============================] - 1s 876us/step - loss: 9.1723e-04 - acc: 0.0000e+00 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 9/25\n",
      "1017/1017 [==============================] - 1s 762us/step - loss: 0.0010 - acc: 0.0000e+00 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 10/25\n",
      "1017/1017 [==============================] - 1s 724us/step - loss: 0.0010 - acc: 0.0000e+00 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 11/25\n",
      "1017/1017 [==============================] - 1s 764us/step - loss: 0.0010 - acc: 0.0000e+00 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 12/25\n",
      "1017/1017 [==============================] - 1s 743us/step - loss: 9.8356e-04 - acc: 0.0000e+00 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 13/25\n",
      "1017/1017 [==============================] - 1s 761us/step - loss: 0.0011 - acc: 0.0000e+00 - val_loss: 0.0017 - val_acc: 0.0000e+00\n",
      "Epoch 14/25\n",
      "1017/1017 [==============================] - 1s 757us/step - loss: 0.0010 - acc: 0.0000e+00 - val_loss: 8.9671e-04 - val_acc: 0.0000e+00\n",
      "Epoch 15/25\n",
      "1017/1017 [==============================] - 1s 763us/step - loss: 9.8565e-04 - acc: 0.0000e+00 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 16/25\n",
      "1017/1017 [==============================] - 1s 746us/step - loss: 8.9983e-04 - acc: 0.0000e+00 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 17/25\n",
      "1017/1017 [==============================] - 1s 734us/step - loss: 9.4695e-04 - acc: 0.0000e+00 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 18/25\n",
      "1017/1017 [==============================] - 1s 751us/step - loss: 0.0011 - acc: 0.0000e+00 - val_loss: 0.0010 - val_acc: 0.0000e+00\n",
      "Epoch 19/25\n",
      "1017/1017 [==============================] - 1s 733us/step - loss: 0.0011 - acc: 0.0000e+00 - val_loss: 9.5859e-04 - val_acc: 0.0000e+00\n",
      "Epoch 20/25\n",
      "1017/1017 [==============================] - 1s 737us/step - loss: 9.1382e-04 - acc: 0.0000e+00 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 21/25\n",
      "1017/1017 [==============================] - 1s 814us/step - loss: 0.0010 - acc: 0.0000e+00 - val_loss: 7.4721e-04 - val_acc: 0.0000e+00\n",
      "Epoch 22/25\n",
      "1017/1017 [==============================] - 1s 789us/step - loss: 9.4992e-04 - acc: 0.0000e+00 - val_loss: 8.5533e-04 - val_acc: 0.0000e+00\n",
      "Epoch 23/25\n",
      "1017/1017 [==============================] - 1s 748us/step - loss: 0.0010 - acc: 0.0000e+00 - val_loss: 8.9111e-04 - val_acc: 0.0000e+00\n",
      "Epoch 24/25\n",
      "1017/1017 [==============================] - 1s 796us/step - loss: 9.9114e-04 - acc: 0.0000e+00 - val_loss: 5.1859e-04 - val_acc: 0.0000e+00\n",
      "Epoch 25/25\n",
      "1017/1017 [==============================] - 1s 870us/step - loss: 9.8505e-04 - acc: 0.0000e+00 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Train on 1017 samples, validate on 113 samples\n",
      "Epoch 1/25\n",
      "1017/1017 [==============================] - 1s 738us/step - loss: 9.6536e-04 - acc: 0.0000e+00 - val_loss: 6.0625e-04 - val_acc: 0.0000e+00\n",
      "Epoch 2/25\n",
      "1017/1017 [==============================] - 1s 766us/step - loss: 9.5133e-04 - acc: 0.0000e+00 - val_loss: 8.8396e-04 - val_acc: 0.0000e+00\n",
      "Epoch 3/25\n",
      "1017/1017 [==============================] - 1s 734us/step - loss: 9.3514e-04 - acc: 0.0000e+00 - val_loss: 8.4092e-04 - val_acc: 0.0000e+00\n",
      "Epoch 4/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1017/1017 [==============================] - 1s 772us/step - loss: 9.9790e-04 - acc: 0.0000e+00 - val_loss: 8.8260e-04 - val_acc: 0.0000e+00\n",
      "Epoch 5/25\n",
      "1017/1017 [==============================] - 1s 733us/step - loss: 0.0011 - acc: 0.0000e+00 - val_loss: 9.7503e-04 - val_acc: 0.0000e+00\n",
      "Epoch 6/25\n",
      "1017/1017 [==============================] - 1s 712us/step - loss: 0.0011 - acc: 0.0000e+00 - val_loss: 5.8435e-04 - val_acc: 0.0000e+00\n",
      "Epoch 7/25\n",
      "1017/1017 [==============================] - 1s 708us/step - loss: 9.5124e-04 - acc: 0.0000e+00 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 8/25\n",
      "1017/1017 [==============================] - 1s 729us/step - loss: 9.6816e-04 - acc: 0.0000e+00 - val_loss: 4.1202e-04 - val_acc: 0.0000e+00\n",
      "Epoch 9/25\n",
      "1017/1017 [==============================] - 1s 762us/step - loss: 9.5216e-04 - acc: 0.0000e+00 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 10/25\n",
      "1017/1017 [==============================] - 1s 759us/step - loss: 9.7180e-04 - acc: 0.0000e+00 - val_loss: 5.3477e-04 - val_acc: 0.0000e+00\n",
      "Epoch 11/25\n",
      "1017/1017 [==============================] - 1s 771us/step - loss: 0.0010 - acc: 0.0000e+00 - val_loss: 8.6272e-04 - val_acc: 0.0000e+00\n",
      "Epoch 12/25\n",
      "1017/1017 [==============================] - 1s 726us/step - loss: 0.0010 - acc: 0.0000e+00 - val_loss: 7.6716e-04 - val_acc: 0.0000e+00\n",
      "Epoch 13/25\n",
      "1017/1017 [==============================] - 1s 756us/step - loss: 0.0010 - acc: 0.0000e+00 - val_loss: 5.6242e-04 - val_acc: 0.0000e+00\n",
      "Epoch 14/25\n",
      "1017/1017 [==============================] - 1s 760us/step - loss: 0.0010 - acc: 0.0000e+00 - val_loss: 0.0013 - val_acc: 0.0000e+00\n",
      "Epoch 15/25\n",
      "1017/1017 [==============================] - 1s 708us/step - loss: 9.3417e-04 - acc: 0.0000e+00 - val_loss: 4.2232e-04 - val_acc: 0.0000e+00\n",
      "Epoch 16/25\n",
      "1017/1017 [==============================] - 1s 735us/step - loss: 9.1131e-04 - acc: 0.0000e+00 - val_loss: 0.0014 - val_acc: 0.0000e+00\n",
      "Epoch 17/25\n",
      "1017/1017 [==============================] - 1s 689us/step - loss: 9.9644e-04 - acc: 0.0000e+00 - val_loss: 5.5826e-04 - val_acc: 0.0000e+00\n",
      "Epoch 18/25\n",
      "1017/1017 [==============================] - 1s 752us/step - loss: 9.8640e-04 - acc: 0.0000e+00 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 19/25\n",
      "1017/1017 [==============================] - 1s 732us/step - loss: 9.2824e-04 - acc: 0.0000e+00 - val_loss: 8.5606e-04 - val_acc: 0.0000e+00\n",
      "Epoch 20/25\n",
      "1017/1017 [==============================] - 1s 716us/step - loss: 9.1193e-04 - acc: 0.0000e+00 - val_loss: 6.7036e-04 - val_acc: 0.0000e+00\n",
      "Epoch 21/25\n",
      "1017/1017 [==============================] - 1s 730us/step - loss: 9.2584e-04 - acc: 0.0000e+00 - val_loss: 0.0012 - val_acc: 0.0000e+00\n",
      "Epoch 22/25\n",
      "1017/1017 [==============================] - 1s 695us/step - loss: 9.2377e-04 - acc: 0.0000e+00 - val_loss: 5.4415e-04 - val_acc: 0.0000e+00\n",
      "Epoch 23/25\n",
      "1017/1017 [==============================] - 1s 736us/step - loss: 9.4077e-04 - acc: 0.0000e+00 - val_loss: 8.4969e-04 - val_acc: 0.0000e+00\n",
      "Epoch 24/25\n",
      "1017/1017 [==============================] - 1s 765us/step - loss: 9.3521e-04 - acc: 0.0000e+00 - val_loss: 5.4758e-04 - val_acc: 0.0000e+00\n",
      "Epoch 25/25\n",
      "1017/1017 [==============================] - 1s 756us/step - loss: 9.5128e-04 - acc: 0.0000e+00 - val_loss: 7.5942e-04 - val_acc: 0.0000e+00\n",
      "Train on 1017 samples, validate on 113 samples\n",
      "Epoch 1/25\n",
      "1017/1017 [==============================] - 1s 735us/step - loss: 9.6599e-04 - acc: 0.0000e+00 - val_loss: 3.9351e-04 - val_acc: 0.0000e+00\n",
      "Epoch 2/25\n",
      "1017/1017 [==============================] - 1s 929us/step - loss: 8.6550e-04 - acc: 0.0000e+00 - val_loss: 7.4395e-04 - val_acc: 0.0000e+00\n",
      "Epoch 3/25\n",
      "1017/1017 [==============================] - 1s 796us/step - loss: 8.5985e-04 - acc: 0.0000e+00 - val_loss: 4.3998e-04 - val_acc: 0.0000e+00\n",
      "Epoch 4/25\n",
      "1017/1017 [==============================] - 1s 794us/step - loss: 8.9891e-04 - acc: 0.0000e+00 - val_loss: 5.4075e-04 - val_acc: 0.0000e+00\n",
      "Epoch 5/25\n",
      "1017/1017 [==============================] - 1s 734us/step - loss: 8.5747e-04 - acc: 0.0000e+00 - val_loss: 4.5649e-04 - val_acc: 0.0000e+00\n",
      "Epoch 6/25\n",
      "1017/1017 [==============================] - 1s 816us/step - loss: 9.0938e-04 - acc: 0.0000e+00 - val_loss: 5.9092e-04 - val_acc: 0.0000e+00\n",
      "Epoch 7/25\n",
      "1017/1017 [==============================] - 1s 743us/step - loss: 9.0564e-04 - acc: 0.0000e+00 - val_loss: 5.1097e-04 - val_acc: 0.0000e+00\n",
      "Epoch 8/25\n",
      "1017/1017 [==============================] - 1s 828us/step - loss: 8.8213e-04 - acc: 0.0000e+00 - val_loss: 3.1697e-04 - val_acc: 0.0000e+00\n",
      "Epoch 9/25\n",
      "1017/1017 [==============================] - 1s 862us/step - loss: 8.9462e-04 - acc: 0.0000e+00 - val_loss: 6.5663e-04 - val_acc: 0.0000e+00\n",
      "Epoch 10/25\n",
      "1017/1017 [==============================] - 1s 850us/step - loss: 8.5787e-04 - acc: 0.0000e+00 - val_loss: 2.6529e-04 - val_acc: 0.0000e+00\n",
      "Epoch 11/25\n",
      "1017/1017 [==============================] - 1s 750us/step - loss: 8.5943e-04 - acc: 0.0000e+00 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 12/25\n",
      "1017/1017 [==============================] - 1s 862us/step - loss: 9.7782e-04 - acc: 0.0000e+00 - val_loss: 2.9284e-04 - val_acc: 0.0000e+00\n",
      "Epoch 13/25\n",
      "1017/1017 [==============================] - 1s 712us/step - loss: 8.6566e-04 - acc: 0.0000e+00 - val_loss: 7.3042e-04 - val_acc: 0.0000e+00\n",
      "Epoch 14/25\n",
      "1017/1017 [==============================] - 1s 720us/step - loss: 8.3492e-04 - acc: 0.0000e+00 - val_loss: 2.8966e-04 - val_acc: 0.0000e+00\n",
      "Epoch 15/25\n",
      "1017/1017 [==============================] - 1s 727us/step - loss: 8.6449e-04 - acc: 0.0000e+00 - val_loss: 6.6447e-04 - val_acc: 0.0000e+00\n",
      "Epoch 16/25\n",
      "1017/1017 [==============================] - 1s 709us/step - loss: 8.4789e-04 - acc: 0.0000e+00 - val_loss: 4.6868e-04 - val_acc: 0.0000e+00\n",
      "Epoch 17/25\n",
      "1017/1017 [==============================] - 1s 702us/step - loss: 8.3536e-04 - acc: 0.0000e+00 - val_loss: 3.1814e-04 - val_acc: 0.0000e+00\n",
      "Epoch 18/25\n",
      "1017/1017 [==============================] - 1s 707us/step - loss: 8.4890e-04 - acc: 0.0000e+00 - val_loss: 7.7854e-04 - val_acc: 0.0000e+00\n",
      "Epoch 19/25\n",
      "1017/1017 [==============================] - 1s 705us/step - loss: 8.7359e-04 - acc: 0.0000e+00 - val_loss: 2.4156e-04 - val_acc: 0.0000e+00\n",
      "Epoch 20/25\n",
      "1017/1017 [==============================] - 1s 858us/step - loss: 8.7648e-04 - acc: 0.0000e+00 - val_loss: 7.9522e-04 - val_acc: 0.0000e+00\n",
      "Epoch 21/25\n",
      "1017/1017 [==============================] - 1s 809us/step - loss: 9.6467e-04 - acc: 0.0000e+00 - val_loss: 2.2572e-04 - val_acc: 0.0000e+00\n",
      "Epoch 22/25\n",
      "1017/1017 [==============================] - 1s 691us/step - loss: 9.8833e-04 - acc: 0.0000e+00 - val_loss: 0.0011 - val_acc: 0.0000e+00\n",
      "Epoch 23/25\n",
      "1017/1017 [==============================] - 1s 747us/step - loss: 0.0010 - acc: 0.0000e+00 - val_loss: 2.2842e-04 - val_acc: 0.0000e+00\n",
      "Epoch 24/25\n",
      "1017/1017 [==============================] - 1s 741us/step - loss: 9.3914e-04 - acc: 0.0000e+00 - val_loss: 9.3120e-04 - val_acc: 0.0000e+00\n",
      "Epoch 25/25\n",
      "1017/1017 [==============================] - 1s 717us/step - loss: 0.0010 - acc: 0.0000e+00 - val_loss: 2.0352e-04 - val_acc: 0.0000e+00\n",
      "Train on 1017 samples, validate on 113 samples\n",
      "Epoch 1/25\n",
      "1017/1017 [==============================] - 1s 748us/step - loss: 9.1310e-04 - acc: 0.0000e+00 - val_loss: 6.3991e-04 - val_acc: 0.0000e+00\n",
      "Epoch 2/25\n",
      "1017/1017 [==============================] - 1s 717us/step - loss: 8.4794e-04 - acc: 0.0000e+00 - val_loss: 3.8441e-04 - val_acc: 0.0000e+00\n",
      "Epoch 3/25\n",
      "1017/1017 [==============================] - 1s 709us/step - loss: 8.3463e-04 - acc: 0.0000e+00 - val_loss: 4.3094e-04 - val_acc: 0.0000e+00\n",
      "Epoch 4/25\n",
      "1017/1017 [==============================] - 1s 714us/step - loss: 8.2491e-04 - acc: 0.0000e+00 - val_loss: 4.5265e-04 - val_acc: 0.0000e+00\n",
      "Epoch 5/25\n",
      "1017/1017 [==============================] - 1s 734us/step - loss: 8.3941e-04 - acc: 0.0000e+00 - val_loss: 4.1748e-04 - val_acc: 0.0000e+00\n",
      "Epoch 6/25\n",
      "1017/1017 [==============================] - 1s 723us/step - loss: 8.6462e-04 - acc: 0.0000e+00 - val_loss: 3.5290e-04 - val_acc: 0.0000e+00\n",
      "Epoch 7/25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1017/1017 [==============================] - 1s 735us/step - loss: 8.3451e-04 - acc: 0.0000e+00 - val_loss: 3.7533e-04 - val_acc: 0.0000e+00\n",
      "Epoch 8/25\n",
      "1017/1017 [==============================] - 1s 713us/step - loss: 8.6679e-04 - acc: 0.0000e+00 - val_loss: 2.6609e-04 - val_acc: 0.0000e+00\n",
      "Epoch 9/25\n",
      "1017/1017 [==============================] - 1s 758us/step - loss: 8.1092e-04 - acc: 0.0000e+00 - val_loss: 2.4991e-04 - val_acc: 0.0000e+00\n",
      "Epoch 10/25\n",
      "1017/1017 [==============================] - 1s 718us/step - loss: 8.1001e-04 - acc: 0.0000e+00 - val_loss: 2.5880e-04 - val_acc: 0.0000e+00\n",
      "Epoch 11/25\n",
      "1017/1017 [==============================] - 1s 746us/step - loss: 8.6757e-04 - acc: 0.0000e+00 - val_loss: 2.7184e-04 - val_acc: 0.0000e+00\n",
      "Epoch 12/25\n",
      "1017/1017 [==============================] - 1s 732us/step - loss: 8.9637e-04 - acc: 0.0000e+00 - val_loss: 2.1658e-04 - val_acc: 0.0000e+00\n",
      "Epoch 13/25\n",
      "1017/1017 [==============================] - 1s 707us/step - loss: 8.4981e-04 - acc: 0.0000e+00 - val_loss: 2.3900e-04 - val_acc: 0.0000e+00\n",
      "Epoch 14/25\n",
      "1017/1017 [==============================] - 1s 734us/step - loss: 9.0877e-04 - acc: 0.0000e+00 - val_loss: 2.4239e-04 - val_acc: 0.0000e+00\n",
      "Epoch 15/25\n",
      "1017/1017 [==============================] - 1s 700us/step - loss: 8.7047e-04 - acc: 0.0000e+00 - val_loss: 2.7581e-04 - val_acc: 0.0000e+00\n",
      "Epoch 16/25\n",
      "1017/1017 [==============================] - 1s 714us/step - loss: 8.0745e-04 - acc: 0.0000e+00 - val_loss: 2.6851e-04 - val_acc: 0.0000e+00\n",
      "Epoch 17/25\n",
      "1017/1017 [==============================] - 1s 720us/step - loss: 7.6710e-04 - acc: 0.0000e+00 - val_loss: 2.2791e-04 - val_acc: 0.0000e+00\n",
      "Epoch 18/25\n",
      "1017/1017 [==============================] - 1s 718us/step - loss: 8.3676e-04 - acc: 0.0000e+00 - val_loss: 2.3113e-04 - val_acc: 0.0000e+00\n",
      "Epoch 19/25\n",
      "1017/1017 [==============================] - 1s 699us/step - loss: 7.9102e-04 - acc: 0.0000e+00 - val_loss: 2.0007e-04 - val_acc: 0.0000e+00\n",
      "Epoch 20/25\n",
      "1017/1017 [==============================] - 1s 701us/step - loss: 8.5598e-04 - acc: 0.0000e+00 - val_loss: 2.0500e-04 - val_acc: 0.0000e+00\n",
      "Epoch 21/25\n",
      "1017/1017 [==============================] - 1s 699us/step - loss: 8.7939e-04 - acc: 0.0000e+00 - val_loss: 2.7762e-04 - val_acc: 0.0000e+00\n",
      "Epoch 22/25\n",
      "1017/1017 [==============================] - 1s 741us/step - loss: 8.2344e-04 - acc: 0.0000e+00 - val_loss: 2.0999e-04 - val_acc: 0.0000e+00\n",
      "Epoch 23/25\n",
      "1017/1017 [==============================] - 1s 737us/step - loss: 8.4591e-04 - acc: 0.0000e+00 - val_loss: 2.7771e-04 - val_acc: 0.0000e+00\n",
      "Epoch 24/25\n",
      "1017/1017 [==============================] - 1s 753us/step - loss: 7.9740e-04 - acc: 0.0000e+00 - val_loss: 2.0366e-04 - val_acc: 0.0000e+00\n",
      "Epoch 25/25\n",
      "1017/1017 [==============================] - 1s 900us/step - loss: 8.4064e-04 - acc: 0.0000e+00 - val_loss: 3.9674e-04 - val_acc: 0.0000e+00\n",
      "Train on 1017 samples, validate on 113 samples\n",
      "Epoch 1/25\n",
      "1017/1017 [==============================] - 1s 930us/step - loss: 8.6625e-04 - acc: 0.0000e+00 - val_loss: 1.9898e-04 - val_acc: 0.0000e+00\n",
      "Epoch 2/25\n",
      "1017/1017 [==============================] - 1s 893us/step - loss: 8.5316e-04 - acc: 0.0000e+00 - val_loss: 2.2325e-04 - val_acc: 0.0000e+00\n",
      "Epoch 3/25\n",
      "1017/1017 [==============================] - 1s 873us/step - loss: 8.2882e-04 - acc: 0.0000e+00 - val_loss: 3.3497e-04 - val_acc: 0.0000e+00\n",
      "Epoch 4/25\n",
      "1017/1017 [==============================] - 1s 911us/step - loss: 8.5941e-04 - acc: 0.0000e+00 - val_loss: 1.9302e-04 - val_acc: 0.0000e+00\n",
      "Epoch 5/25\n",
      "1017/1017 [==============================] - 1s 760us/step - loss: 8.0798e-04 - acc: 0.0000e+00 - val_loss: 2.5616e-04 - val_acc: 0.0000e+00\n",
      "Epoch 6/25\n",
      "1017/1017 [==============================] - 1s 790us/step - loss: 7.7118e-04 - acc: 0.0000e+00 - val_loss: 1.9396e-04 - val_acc: 0.0000e+00\n",
      "Epoch 7/25\n",
      "1017/1017 [==============================] - 1s 808us/step - loss: 7.8973e-04 - acc: 0.0000e+00 - val_loss: 1.9436e-04 - val_acc: 0.0000e+00\n",
      "Epoch 8/25\n",
      "1017/1017 [==============================] - 1s 763us/step - loss: 8.0272e-04 - acc: 0.0000e+00 - val_loss: 1.9804e-04 - val_acc: 0.0000e+00\n",
      "Epoch 9/25\n",
      "1017/1017 [==============================] - 1s 729us/step - loss: 7.8122e-04 - acc: 0.0000e+00 - val_loss: 1.9453e-04 - val_acc: 0.0000e+00\n",
      "Epoch 10/25\n",
      "1017/1017 [==============================] - 1s 710us/step - loss: 8.4188e-04 - acc: 0.0000e+00 - val_loss: 2.0711e-04 - val_acc: 0.0000e+00\n",
      "Epoch 11/25\n",
      "1017/1017 [==============================] - 1s 709us/step - loss: 8.2049e-04 - acc: 0.0000e+00 - val_loss: 2.2571e-04 - val_acc: 0.0000e+00\n",
      "Epoch 12/25\n",
      "1017/1017 [==============================] - 1s 712us/step - loss: 8.2727e-04 - acc: 0.0000e+00 - val_loss: 1.9847e-04 - val_acc: 0.0000e+00\n",
      "Epoch 13/25\n",
      "1017/1017 [==============================] - 1s 738us/step - loss: 8.0794e-04 - acc: 0.0000e+00 - val_loss: 1.9384e-04 - val_acc: 0.0000e+00\n",
      "Epoch 14/25\n",
      "1017/1017 [==============================] - 1s 709us/step - loss: 7.8920e-04 - acc: 0.0000e+00 - val_loss: 2.9621e-04 - val_acc: 0.0000e+00\n",
      "Epoch 15/25\n",
      "1017/1017 [==============================] - 1s 714us/step - loss: 7.4412e-04 - acc: 0.0000e+00 - val_loss: 1.9833e-04 - val_acc: 0.0000e+00\n",
      "Epoch 16/25\n",
      "1017/1017 [==============================] - 1s 719us/step - loss: 8.0783e-04 - acc: 0.0000e+00 - val_loss: 1.9619e-04 - val_acc: 0.0000e+00\n",
      "Epoch 17/25\n",
      "1017/1017 [==============================] - 1s 757us/step - loss: 7.7796e-04 - acc: 0.0000e+00 - val_loss: 2.2850e-04 - val_acc: 0.0000e+00\n",
      "Epoch 18/25\n",
      "1017/1017 [==============================] - 1s 763us/step - loss: 7.7715e-04 - acc: 0.0000e+00 - val_loss: 2.1871e-04 - val_acc: 0.0000e+00\n",
      "Epoch 19/25\n",
      "1017/1017 [==============================] - 1s 725us/step - loss: 8.6208e-04 - acc: 0.0000e+00 - val_loss: 1.9182e-04 - val_acc: 0.0000e+00\n",
      "Epoch 20/25\n",
      "1017/1017 [==============================] - 1s 726us/step - loss: 7.7017e-04 - acc: 0.0000e+00 - val_loss: 2.2084e-04 - val_acc: 0.0000e+00\n",
      "Epoch 21/25\n",
      "1017/1017 [==============================] - 1s 750us/step - loss: 8.0685e-04 - acc: 0.0000e+00 - val_loss: 2.0860e-04 - val_acc: 0.0000e+00\n",
      "Epoch 22/25\n",
      "1017/1017 [==============================] - 1s 781us/step - loss: 7.7646e-04 - acc: 0.0000e+00 - val_loss: 3.4811e-04 - val_acc: 0.0000e+00\n",
      "Epoch 23/25\n",
      "1017/1017 [==============================] - 1s 722us/step - loss: 8.0708e-04 - acc: 0.0000e+00 - val_loss: 2.9841e-04 - val_acc: 0.0000e+00\n",
      "Epoch 24/25\n",
      "1017/1017 [==============================] - 1s 764us/step - loss: 8.5561e-04 - acc: 0.0000e+00 - val_loss: 4.7324e-04 - val_acc: 0.0000e+00\n",
      "Epoch 25/25\n",
      "1017/1017 [==============================] - 1s 756us/step - loss: 7.8647e-04 - acc: 0.0000e+00 - val_loss: 2.7021e-04 - val_acc: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if action == 'train':    \n",
    "    run_date = datetime.datetime.now()\n",
    "    model = build_model2(shape, neurons, d)\n",
    "    save_plot_epochs = 25\n",
    "    for i in range(int(epochs / save_plot_epochs)):\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            batch_size=512,\n",
    "            epochs=save_plot_epochs,\n",
    "            validation_split=0.1,\n",
    "            verbose=1)\n",
    "        predict_y = model.predict(X_test)\n",
    "        plot_result(stock_name, predict_y, y_test, save_plot_epochs * (i + 1), run_date)\n",
    "        plot_value_change_and_revenue(stock_name, predict_y, y_test, save_plot_epochs * (i + 1), run_date)\n",
    "    model.save('LSTM_Stock_prediction.h5')\n",
    "    \n",
    "elif action == 'predict':\n",
    "    load_model('LSTM_Stock_prediction.h5')\n",
    "    predict_y = model.predict(X_test)\n",
    "else:\n",
    "    print('use train or predict')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
