{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock value prediction from Open, High, Low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Import module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt2\n",
    "import pandas as pd\n",
    "from pandas import datetime\n",
    "import math, time\n",
    "import itertools\n",
    "from sklearn import preprocessing\n",
    "import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import load_model\n",
    "import keras\n",
    "import pandas_datareader.data as web\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_name = '^GSPC'\n",
    "seq_len = 22\n",
    "d = 0.2\n",
    "shape = [4, seq_len, 1] # feature, window, output\n",
    "neurons = [128, 128, 32, 1]\n",
    "epochs = 150\n",
    "\n",
    "start = datetime.datetime(2013, 1, 1)\n",
    "end = datetime.date.today()\n",
    "\n",
    "optimizer = 'RMSprop'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1. Download data and normalize it\n",
    "Data since 1950 to today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_data(stock_name, normalize=True):\n",
    "    \n",
    "    df = web.DataReader(stock_name, \"yahoo\", start, end)\n",
    "    df.drop(['Volume', 'Close'], 1, inplace=True)\n",
    "    \n",
    "    if normalize:\n",
    "#         yesterday_value = pd.concat([df[:1], df[:len(df)-1]], ignore_index=True)\n",
    "#         df = pd.concat([df[:]], ignore_index=True)\n",
    "#         df = df / yesterday_value\n",
    "        \n",
    "        min_max_scaler = preprocessing.MinMaxScaler()\n",
    "        df['Open'] = min_max_scaler.fit_transform(df.Open.values.reshape(-1,1))\n",
    "        df['High'] = min_max_scaler.fit_transform(df.High.values.reshape(-1,1))\n",
    "        df['Low'] = min_max_scaler.fit_transform(df.Low.values.reshape(-1,1))\n",
    "        df['Adj Close'] = min_max_scaler.fit_transform(df['Adj Close'].values.reshape(-1,1))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_stock_data(stock_name, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Plot out the Normalized Adjusted close price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stock(stock_name):\n",
    "    df = get_stock_data(stock_name, normalize=True)\n",
    "    print(df.head())\n",
    "    plt.plot(df['Adj Close'], color='red', label='Adj Close')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Open      High       Low  Adj Close\n",
      "Date                                               \n",
      "2012-12-31  0.000000  0.000000  0.000000   0.000000\n",
      "2013-01-02  0.018415  0.028142  0.021804   0.028664\n",
      "2013-01-03  0.046494  0.030539  0.044587   0.026251\n",
      "2013-01-04  0.044130  0.032486  0.047274   0.031868\n",
      "2013-01-07  0.049633  0.031327  0.045434   0.028244\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAEJCAYAAAB11IfBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11a1d9fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_stock(stock_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Set last day Adjusted Close as y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(stock, seq_len):\n",
    "    amount_of_features = len(stock.columns)\n",
    "    data = stock.as_matrix() \n",
    "    sequence_length = seq_len + 1 # index starting from 0\n",
    "    result = []\n",
    "    \n",
    "    for index in range(len(data) - sequence_length): # maxmimum date = lastest date - sequence length\n",
    "        result.append(data[index: index + sequence_length]) # index : index + 22days\n",
    "    \n",
    "    result = np.array(result)\n",
    "    row = round(0.9 * result.shape[0]) # 90% split\n",
    "    \n",
    "    train = result[:int(row), :] # 90% date\n",
    "    X_train = train[:, :-1] # all data until day m\n",
    "    y_train = train[:, -1][:,-1] # day m + 1 adjusted close price\n",
    "    \n",
    "    X_test = result[int(row):, :-1]\n",
    "    y_test = result[int(row):, -1][:,-1] \n",
    "\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], amount_of_features))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], amount_of_features))  \n",
    "\n",
    "    return [X_train, y_train, X_test, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = load_data(df, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1112, 22, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[0], X_train.shape[1], X_train.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1112"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Buidling neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model2(layers, neurons, d):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(neurons[0], input_shape=(layers[1], layers[0]), return_sequences=True))\n",
    "    model.add(Dropout(d))\n",
    "        \n",
    "    model.add(LSTM(neurons[1], input_shape=(layers[1], layers[0]), return_sequences=False))\n",
    "    model.add(Dropout(d))\n",
    "        \n",
    "    model.add(Dense(neurons[2],kernel_initializer=\"uniform\",activation='relu'))        \n",
    "    model.add(Dense(neurons[3],kernel_initializer=\"uniform\",activation='linear'))\n",
    "    # model = load_model('my_LSTM_stock_model1000.h5')\n",
    "    # adam = keras.optimizers.Adam(decay=0.2)\n",
    "    model.compile(loss='mse',optimizer=optimizer , metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Model Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 22, 128)           68096     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 22, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 203,841\n",
      "Trainable params: 203,841\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model2(shape, neurons, d)\n",
    "# layers = [4, 22, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 112 samples\n",
      "Epoch 1/150\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1867 - acc: 0.0000e+00 - val_loss: 0.4537 - val_acc: 0.0000e+00\n",
      "Epoch 2/150\n",
      "1000/1000 [==============================] - 1s 698us/step - loss: 0.1051 - acc: 0.0000e+00 - val_loss: 0.0719 - val_acc: 0.0000e+00\n",
      "Epoch 3/150\n",
      "1000/1000 [==============================] - 1s 693us/step - loss: 0.0189 - acc: 0.0000e+00 - val_loss: 0.0635 - val_acc: 0.0000e+00\n",
      "Epoch 4/150\n",
      "1000/1000 [==============================] - 1s 756us/step - loss: 0.0110 - acc: 0.0000e+00 - val_loss: 0.0372 - val_acc: 0.0000e+00\n",
      "Epoch 5/150\n",
      "1000/1000 [==============================] - 1s 737us/step - loss: 0.0168 - acc: 0.0000e+00 - val_loss: 0.0309 - val_acc: 0.0000e+00\n",
      "Epoch 6/150\n",
      "1000/1000 [==============================] - 1s 725us/step - loss: 0.0171 - acc: 0.0000e+00 - val_loss: 0.0628 - val_acc: 0.0000e+00\n",
      "Epoch 7/150\n",
      "1000/1000 [==============================] - 1s 730us/step - loss: 0.0059 - acc: 0.0000e+00 - val_loss: 0.0507 - val_acc: 0.0000e+00\n",
      "Epoch 8/150\n",
      "1000/1000 [==============================] - 1s 717us/step - loss: 0.0036 - acc: 0.0000e+00 - val_loss: 0.0359 - val_acc: 0.0000e+00\n",
      "Epoch 9/150\n",
      "1000/1000 [==============================] - 1s 742us/step - loss: 0.0058 - acc: 0.0000e+00 - val_loss: 0.0095 - val_acc: 0.0000e+00\n",
      "Epoch 10/150\n",
      "1000/1000 [==============================] - 1s 723us/step - loss: 0.0270 - acc: 0.0000e+00 - val_loss: 0.0471 - val_acc: 0.0000e+00\n",
      "Epoch 11/150\n",
      "1000/1000 [==============================] - 1s 695us/step - loss: 0.0070 - acc: 0.0000e+00 - val_loss: 0.0576 - val_acc: 0.0000e+00\n",
      "Epoch 12/150\n",
      "1000/1000 [==============================] - 1s 738us/step - loss: 0.0039 - acc: 0.0000e+00 - val_loss: 0.0475 - val_acc: 0.0000e+00\n",
      "Epoch 13/150\n",
      "1000/1000 [==============================] - 1s 747us/step - loss: 0.0030 - acc: 0.0000e+00 - val_loss: 0.0436 - val_acc: 0.0000e+00\n",
      "Epoch 14/150\n",
      "1000/1000 [==============================] - 1s 755us/step - loss: 0.0032 - acc: 0.0000e+00 - val_loss: 0.0485 - val_acc: 0.0000e+00\n",
      "Epoch 15/150\n",
      "1000/1000 [==============================] - 1s 779us/step - loss: 0.0097 - acc: 0.0000e+00 - val_loss: 0.0938 - val_acc: 0.0000e+00\n",
      "Epoch 16/150\n",
      "1000/1000 [==============================] - 1s 728us/step - loss: 0.0179 - acc: 0.0000e+00 - val_loss: 0.0749 - val_acc: 0.0000e+00\n",
      "Epoch 17/150\n",
      "1000/1000 [==============================] - 1s 720us/step - loss: 0.0050 - acc: 0.0000e+00 - val_loss: 0.0534 - val_acc: 0.0000e+00\n",
      "Epoch 18/150\n",
      "1000/1000 [==============================] - 1s 731us/step - loss: 0.0035 - acc: 0.0000e+00 - val_loss: 0.0493 - val_acc: 0.0000e+00\n",
      "Epoch 19/150\n",
      "1000/1000 [==============================] - 1s 752us/step - loss: 0.0034 - acc: 0.0000e+00 - val_loss: 0.0500 - val_acc: 0.0000e+00\n",
      "Epoch 20/150\n",
      "1000/1000 [==============================] - 1s 782us/step - loss: 0.0054 - acc: 0.0000e+00 - val_loss: 0.0672 - val_acc: 0.0000e+00\n",
      "Epoch 21/150\n",
      "1000/1000 [==============================] - 1s 743us/step - loss: 0.0126 - acc: 0.0000e+00 - val_loss: 0.0805 - val_acc: 0.0000e+00\n",
      "Epoch 22/150\n",
      "1000/1000 [==============================] - 1s 769us/step - loss: 0.0078 - acc: 0.0000e+00 - val_loss: 0.0607 - val_acc: 0.0000e+00\n",
      "Epoch 23/150\n",
      "1000/1000 [==============================] - 1s 741us/step - loss: 0.0038 - acc: 0.0000e+00 - val_loss: 0.0471 - val_acc: 0.0000e+00\n",
      "Epoch 24/150\n",
      "1000/1000 [==============================] - 1s 723us/step - loss: 0.0031 - acc: 0.0000e+00 - val_loss: 0.0469 - val_acc: 0.0000e+00\n",
      "Epoch 25/150\n",
      "1000/1000 [==============================] - 1s 811us/step - loss: 0.0054 - acc: 0.0000e+00 - val_loss: 0.0695 - val_acc: 0.0000e+00\n",
      "Epoch 26/150\n",
      "1000/1000 [==============================] - 1s 758us/step - loss: 0.0127 - acc: 0.0000e+00 - val_loss: 0.0785 - val_acc: 0.0000e+00\n",
      "Epoch 27/150\n",
      "1000/1000 [==============================] - 1s 741us/step - loss: 0.0064 - acc: 0.0000e+00 - val_loss: 0.0564 - val_acc: 0.0000e+00\n",
      "Epoch 28/150\n",
      "1000/1000 [==============================] - 1s 762us/step - loss: 0.0036 - acc: 0.0000e+00 - val_loss: 0.0516 - val_acc: 0.0000e+00\n",
      "Epoch 29/150\n",
      "1000/1000 [==============================] - 1s 768us/step - loss: 0.0040 - acc: 0.0000e+00 - val_loss: 0.0575 - val_acc: 0.0000e+00\n",
      "Epoch 30/150\n",
      "1000/1000 [==============================] - 1s 785us/step - loss: 0.0086 - acc: 0.0000e+00 - val_loss: 0.0765 - val_acc: 0.0000e+00\n",
      "Epoch 31/150\n",
      "1000/1000 [==============================] - 1s 757us/step - loss: 0.0102 - acc: 0.0000e+00 - val_loss: 0.0693 - val_acc: 0.0000e+00\n",
      "Epoch 32/150\n",
      "1000/1000 [==============================] - 1s 701us/step - loss: 0.0048 - acc: 0.0000e+00 - val_loss: 0.0492 - val_acc: 0.0000e+00\n",
      "Epoch 33/150\n",
      "1000/1000 [==============================] - 1s 729us/step - loss: 0.0032 - acc: 0.0000e+00 - val_loss: 0.0439 - val_acc: 0.0000e+00\n",
      "Epoch 34/150\n",
      "1000/1000 [==============================] - 1s 701us/step - loss: 0.0030 - acc: 0.0000e+00 - val_loss: 0.0477 - val_acc: 0.0000e+00\n",
      "Epoch 35/150\n",
      "1000/1000 [==============================] - 1s 699us/step - loss: 0.0072 - acc: 0.0000e+00 - val_loss: 0.0784 - val_acc: 0.0000e+00\n",
      "Epoch 36/150\n",
      "1000/1000 [==============================] - 1s 715us/step - loss: 0.0123 - acc: 0.0000e+00 - val_loss: 0.0688 - val_acc: 0.0000e+00\n",
      "Epoch 37/150\n",
      "1000/1000 [==============================] - 1s 766us/step - loss: 0.0045 - acc: 0.0000e+00 - val_loss: 0.0491 - val_acc: 0.0000e+00\n",
      "Epoch 38/150\n",
      "1000/1000 [==============================] - 1s 731us/step - loss: 0.0031 - acc: 0.0000e+00 - val_loss: 0.0434 - val_acc: 0.0000e+00\n",
      "Epoch 39/150\n",
      "1000/1000 [==============================] - 1s 771us/step - loss: 0.0034 - acc: 0.0000e+00 - val_loss: 0.0541 - val_acc: 0.0000e+00\n",
      "Epoch 40/150\n",
      "1000/1000 [==============================] - 1s 762us/step - loss: 0.0102 - acc: 0.0000e+00 - val_loss: 0.0761 - val_acc: 0.0000e+00\n",
      "Epoch 41/150\n",
      "1000/1000 [==============================] - 1s 767us/step - loss: 0.0082 - acc: 0.0000e+00 - val_loss: 0.0586 - val_acc: 0.0000e+00\n",
      "Epoch 42/150\n",
      "1000/1000 [==============================] - 1s 774us/step - loss: 0.0040 - acc: 0.0000e+00 - val_loss: 0.0475 - val_acc: 0.0000e+00\n",
      "Epoch 43/150\n",
      "1000/1000 [==============================] - 1s 757us/step - loss: 0.0031 - acc: 0.0000e+00 - val_loss: 0.0457 - val_acc: 0.0000e+00\n",
      "Epoch 44/150\n",
      "1000/1000 [==============================] - 1s 751us/step - loss: 0.0050 - acc: 0.0000e+00 - val_loss: 0.0658 - val_acc: 0.0000e+00\n",
      "Epoch 45/150\n",
      "1000/1000 [==============================] - 1s 769us/step - loss: 0.0116 - acc: 0.0000e+00 - val_loss: 0.0717 - val_acc: 0.0000e+00\n",
      "Epoch 46/150\n",
      "1000/1000 [==============================] - 1s 775us/step - loss: 0.0060 - acc: 0.0000e+00 - val_loss: 0.0543 - val_acc: 0.0000e+00\n",
      "Epoch 47/150\n",
      "1000/1000 [==============================] - 1s 779us/step - loss: 0.0035 - acc: 0.0000e+00 - val_loss: 0.0451 - val_acc: 0.0000e+00\n",
      "Epoch 48/150\n",
      "1000/1000 [==============================] - 1s 741us/step - loss: 0.0031 - acc: 0.0000e+00 - val_loss: 0.0467 - val_acc: 0.0000e+00\n",
      "Epoch 49/150\n",
      "1000/1000 [==============================] - 1s 783us/step - loss: 0.0051 - acc: 0.0000e+00 - val_loss: 0.0617 - val_acc: 0.0000e+00\n",
      "Epoch 50/150\n",
      "1000/1000 [==============================] - 1s 754us/step - loss: 0.0104 - acc: 0.0000e+00 - val_loss: 0.0717 - val_acc: 0.0000e+00\n",
      "Epoch 51/150\n",
      "1000/1000 [==============================] - 1s 763us/step - loss: 0.0066 - acc: 0.0000e+00 - val_loss: 0.0534 - val_acc: 0.0000e+00\n",
      "Epoch 52/150\n",
      "1000/1000 [==============================] - 1s 755us/step - loss: 0.0034 - acc: 0.0000e+00 - val_loss: 0.0449 - val_acc: 0.0000e+00\n",
      "Epoch 53/150\n",
      "1000/1000 [==============================] - 1s 776us/step - loss: 0.0028 - acc: 0.0000e+00 - val_loss: 0.0389 - val_acc: 0.0000e+00\n",
      "Epoch 54/150\n",
      "1000/1000 [==============================] - 1s 750us/step - loss: 0.0032 - acc: 0.0000e+00 - val_loss: 0.0490 - val_acc: 0.0000e+00\n",
      "Epoch 55/150\n",
      "1000/1000 [==============================] - 1s 735us/step - loss: 0.0085 - acc: 0.0000e+00 - val_loss: 0.0730 - val_acc: 0.0000e+00\n",
      "Epoch 56/150\n",
      "1000/1000 [==============================] - 1s 752us/step - loss: 0.0099 - acc: 0.0000e+00 - val_loss: 0.0639 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/150\n",
      "1000/1000 [==============================] - 1s 802us/step - loss: 0.0046 - acc: 0.0000e+00 - val_loss: 0.0473 - val_acc: 0.0000e+00\n",
      "Epoch 58/150\n",
      "1000/1000 [==============================] - 1s 778us/step - loss: 0.0029 - acc: 0.0000e+00 - val_loss: 0.0404 - val_acc: 0.0000e+00\n",
      "Epoch 59/150\n",
      "1000/1000 [==============================] - 1s 768us/step - loss: 0.0026 - acc: 0.0000e+00 - val_loss: 0.0397 - val_acc: 0.0000e+00\n",
      "Epoch 60/150\n",
      "1000/1000 [==============================] - 1s 771us/step - loss: 0.0050 - acc: 0.0000e+00 - val_loss: 0.0655 - val_acc: 0.0000e+00\n",
      "Epoch 61/150\n",
      "1000/1000 [==============================] - 1s 772us/step - loss: 0.0120 - acc: 0.0000e+00 - val_loss: 0.0707 - val_acc: 0.0000e+00\n",
      "Epoch 62/150\n",
      "1000/1000 [==============================] - 1s 785us/step - loss: 0.0055 - acc: 0.0000e+00 - val_loss: 0.0467 - val_acc: 0.0000e+00\n",
      "Epoch 63/150\n",
      "1000/1000 [==============================] - 1s 795us/step - loss: 0.0028 - acc: 0.0000e+00 - val_loss: 0.0372 - val_acc: 0.0000e+00\n",
      "Epoch 64/150\n",
      "1000/1000 [==============================] - 1s 799us/step - loss: 0.0025 - acc: 0.0000e+00 - val_loss: 0.0345 - val_acc: 0.0000e+00\n",
      "Epoch 65/150\n",
      "1000/1000 [==============================] - 1s 786us/step - loss: 0.0026 - acc: 0.0000e+00 - val_loss: 0.0422 - val_acc: 0.0000e+00\n",
      "Epoch 66/150\n",
      "1000/1000 [==============================] - 1s 750us/step - loss: 0.0071 - acc: 0.0000e+00 - val_loss: 0.0711 - val_acc: 0.0000e+00\n",
      "Epoch 67/150\n",
      "1000/1000 [==============================] - 1s 796us/step - loss: 0.0115 - acc: 0.0000e+00 - val_loss: 0.0617 - val_acc: 0.0000e+00\n",
      "Epoch 68/150\n",
      "1000/1000 [==============================] - 1s 760us/step - loss: 0.0045 - acc: 0.0000e+00 - val_loss: 0.0437 - val_acc: 0.0000e+00\n",
      "Epoch 69/150\n",
      "1000/1000 [==============================] - 1s 774us/step - loss: 0.0027 - acc: 0.0000e+00 - val_loss: 0.0361 - val_acc: 0.0000e+00\n",
      "Epoch 70/150\n",
      "1000/1000 [==============================] - 1s 753us/step - loss: 0.0023 - acc: 0.0000e+00 - val_loss: 0.0338 - val_acc: 0.0000e+00\n",
      "Epoch 71/150\n",
      "1000/1000 [==============================] - 1s 749us/step - loss: 0.0031 - acc: 0.0000e+00 - val_loss: 0.0508 - val_acc: 0.0000e+00\n",
      "Epoch 72/150\n",
      "1000/1000 [==============================] - 1s 751us/step - loss: 0.0087 - acc: 0.0000e+00 - val_loss: 0.0705 - val_acc: 0.0000e+00\n",
      "Epoch 73/150\n",
      "1000/1000 [==============================] - 1s 817us/step - loss: 0.0073 - acc: 0.0000e+00 - val_loss: 0.0528 - val_acc: 0.0000e+00\n",
      "Epoch 74/150\n",
      "1000/1000 [==============================] - 1s 748us/step - loss: 0.0036 - acc: 0.0000e+00 - val_loss: 0.0431 - val_acc: 0.0000e+00\n",
      "Epoch 75/150\n",
      "1000/1000 [==============================] - 1s 772us/step - loss: 0.0026 - acc: 0.0000e+00 - val_loss: 0.0357 - val_acc: 0.0000e+00\n",
      "Epoch 76/150\n",
      "1000/1000 [==============================] - 1s 799us/step - loss: 0.0025 - acc: 0.0000e+00 - val_loss: 0.0390 - val_acc: 0.0000e+00\n",
      "Epoch 77/150\n",
      "1000/1000 [==============================] - 1s 765us/step - loss: 0.0050 - acc: 0.0000e+00 - val_loss: 0.0621 - val_acc: 0.0000e+00\n",
      "Epoch 78/150\n",
      "1000/1000 [==============================] - 1s 791us/step - loss: 0.0112 - acc: 0.0000e+00 - val_loss: 0.0635 - val_acc: 0.0000e+00\n",
      "Epoch 79/150\n",
      "1000/1000 [==============================] - 1s 822us/step - loss: 0.0051 - acc: 0.0000e+00 - val_loss: 0.0412 - val_acc: 0.0000e+00\n",
      "Epoch 80/150\n",
      "1000/1000 [==============================] - 1s 766us/step - loss: 0.0027 - acc: 0.0000e+00 - val_loss: 0.0375 - val_acc: 0.0000e+00\n",
      "Epoch 81/150\n",
      "1000/1000 [==============================] - 1s 804us/step - loss: 0.0026 - acc: 0.0000e+00 - val_loss: 0.0393 - val_acc: 0.0000e+00\n",
      "Epoch 82/150\n",
      "1000/1000 [==============================] - 1s 744us/step - loss: 0.0042 - acc: 0.0000e+00 - val_loss: 0.0544 - val_acc: 0.0000e+00\n",
      "Epoch 83/150\n",
      "1000/1000 [==============================] - 1s 771us/step - loss: 0.0074 - acc: 0.0000e+00 - val_loss: 0.0584 - val_acc: 0.0000e+00\n",
      "Epoch 84/150\n",
      "1000/1000 [==============================] - 1s 747us/step - loss: 0.0057 - acc: 0.0000e+00 - val_loss: 0.0490 - val_acc: 0.0000e+00\n",
      "Epoch 85/150\n",
      "1000/1000 [==============================] - 1s 801us/step - loss: 0.0037 - acc: 0.0000e+00 - val_loss: 0.0432 - val_acc: 0.0000e+00\n",
      "Epoch 86/150\n",
      "1000/1000 [==============================] - 1s 684us/step - loss: 0.0038 - acc: 0.0000e+00 - val_loss: 0.0473 - val_acc: 0.0000e+00\n",
      "Epoch 87/150\n",
      "1000/1000 [==============================] - 1s 696us/step - loss: 0.0050 - acc: 0.0000e+00 - val_loss: 0.0522 - val_acc: 0.0000e+00\n",
      "Epoch 88/150\n",
      "1000/1000 [==============================] - 1s 741us/step - loss: 0.0055 - acc: 0.0000e+00 - val_loss: 0.0511 - val_acc: 0.0000e+00\n",
      "Epoch 89/150\n",
      "1000/1000 [==============================] - 1s 743us/step - loss: 0.0046 - acc: 0.0000e+00 - val_loss: 0.0495 - val_acc: 0.0000e+00\n",
      "Epoch 90/150\n",
      "1000/1000 [==============================] - 1s 782us/step - loss: 0.0042 - acc: 0.0000e+00 - val_loss: 0.0422 - val_acc: 0.0000e+00\n",
      "Epoch 91/150\n",
      "1000/1000 [==============================] - 1s 756us/step - loss: 0.0033 - acc: 0.0000e+00 - val_loss: 0.0409 - val_acc: 0.0000e+00\n",
      "Epoch 92/150\n",
      "1000/1000 [==============================] - 1s 768us/step - loss: 0.0040 - acc: 0.0000e+00 - val_loss: 0.0504 - val_acc: 0.0000e+00\n",
      "Epoch 93/150\n",
      "1000/1000 [==============================] - 1s 768us/step - loss: 0.0067 - acc: 0.0000e+00 - val_loss: 0.0565 - val_acc: 0.0000e+00\n",
      "Epoch 94/150\n",
      "1000/1000 [==============================] - 1s 768us/step - loss: 0.0065 - acc: 0.0000e+00 - val_loss: 0.0539 - val_acc: 0.0000e+00\n",
      "Epoch 95/150\n",
      "1000/1000 [==============================] - 1s 714us/step - loss: 0.0041 - acc: 0.0000e+00 - val_loss: 0.0369 - val_acc: 0.0000e+00\n",
      "Epoch 96/150\n",
      "1000/1000 [==============================] - 1s 775us/step - loss: 0.0024 - acc: 0.0000e+00 - val_loss: 0.0343 - val_acc: 0.0000e+00\n",
      "Epoch 97/150\n",
      "1000/1000 [==============================] - 1s 778us/step - loss: 0.0022 - acc: 0.0000e+00 - val_loss: 0.0299 - val_acc: 0.0000e+00\n",
      "Epoch 98/150\n",
      "1000/1000 [==============================] - 1s 769us/step - loss: 0.0026 - acc: 0.0000e+00 - val_loss: 0.0408 - val_acc: 0.0000e+00\n",
      "Epoch 99/150\n",
      "1000/1000 [==============================] - 1s 765us/step - loss: 0.0084 - acc: 0.0000e+00 - val_loss: 0.0691 - val_acc: 0.0000e+00\n",
      "Epoch 100/150\n",
      "1000/1000 [==============================] - 1s 734us/step - loss: 0.0084 - acc: 0.0000e+00 - val_loss: 0.0444 - val_acc: 0.0000e+00\n",
      "Epoch 101/150\n",
      "1000/1000 [==============================] - 1s 706us/step - loss: 0.0030 - acc: 0.0000e+00 - val_loss: 0.0365 - val_acc: 0.0000e+00\n",
      "Epoch 102/150\n",
      "1000/1000 [==============================] - 1s 691us/step - loss: 0.0027 - acc: 0.0000e+00 - val_loss: 0.0342 - val_acc: 0.0000e+00\n",
      "Epoch 103/150\n",
      "1000/1000 [==============================] - 1s 776us/step - loss: 0.0026 - acc: 0.0000e+00 - val_loss: 0.0320 - val_acc: 0.0000e+00\n",
      "Epoch 104/150\n",
      "1000/1000 [==============================] - 1s 828us/step - loss: 0.0028 - acc: 0.0000e+00 - val_loss: 0.0393 - val_acc: 0.0000e+00\n",
      "Epoch 105/150\n",
      "1000/1000 [==============================] - 1s 780us/step - loss: 0.0057 - acc: 0.0000e+00 - val_loss: 0.0573 - val_acc: 0.0000e+00\n",
      "Epoch 106/150\n",
      "1000/1000 [==============================] - 1s 810us/step - loss: 0.0068 - acc: 0.0000e+00 - val_loss: 0.0456 - val_acc: 0.0000e+00\n",
      "Epoch 107/150\n",
      "1000/1000 [==============================] - 1s 751us/step - loss: 0.0035 - acc: 0.0000e+00 - val_loss: 0.0367 - val_acc: 0.0000e+00\n",
      "Epoch 108/150\n",
      "1000/1000 [==============================] - 1s 804us/step - loss: 0.0029 - acc: 0.0000e+00 - val_loss: 0.0354 - val_acc: 0.0000e+00\n",
      "Epoch 109/150\n",
      "1000/1000 [==============================] - 1s 775us/step - loss: 0.0038 - acc: 0.0000e+00 - val_loss: 0.0468 - val_acc: 0.0000e+00\n",
      "Epoch 110/150\n",
      "1000/1000 [==============================] - 1s 808us/step - loss: 0.0058 - acc: 0.0000e+00 - val_loss: 0.0495 - val_acc: 0.0000e+00\n",
      "Epoch 111/150\n",
      "1000/1000 [==============================] - 1s 784us/step - loss: 0.0045 - acc: 0.0000e+00 - val_loss: 0.0412 - val_acc: 0.0000e+00\n",
      "Epoch 112/150\n",
      "1000/1000 [==============================] - 1s 833us/step - loss: 0.0034 - acc: 0.0000e+00 - val_loss: 0.0369 - val_acc: 0.0000e+00\n",
      "Epoch 113/150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 1s 795us/step - loss: 0.0033 - acc: 0.0000e+00 - val_loss: 0.0384 - val_acc: 0.0000e+00\n",
      "Epoch 114/150\n",
      "1000/1000 [==============================] - 1s 749us/step - loss: 0.0043 - acc: 0.0000e+00 - val_loss: 0.0461 - val_acc: 0.0000e+00\n",
      "Epoch 115/150\n",
      "1000/1000 [==============================] - 1s 719us/step - loss: 0.0056 - acc: 0.0000e+00 - val_loss: 0.0473 - val_acc: 0.0000e+00\n",
      "Epoch 116/150\n",
      "1000/1000 [==============================] - 1s 687us/step - loss: 0.0040 - acc: 0.0000e+00 - val_loss: 0.0365 - val_acc: 0.0000e+00\n",
      "Epoch 117/150\n",
      "1000/1000 [==============================] - 1s 688us/step - loss: 0.0033 - acc: 0.0000e+00 - val_loss: 0.0431 - val_acc: 0.0000e+00\n",
      "Epoch 118/150\n",
      "1000/1000 [==============================] - 1s 681us/step - loss: 0.0047 - acc: 0.0000e+00 - val_loss: 0.0440 - val_acc: 0.0000e+00\n",
      "Epoch 119/150\n",
      "1000/1000 [==============================] - 1s 690us/step - loss: 0.0044 - acc: 0.0000e+00 - val_loss: 0.0417 - val_acc: 0.0000e+00\n",
      "Epoch 120/150\n",
      "1000/1000 [==============================] - 1s 689us/step - loss: 0.0036 - acc: 0.0000e+00 - val_loss: 0.0361 - val_acc: 0.0000e+00\n",
      "Epoch 121/150\n",
      "1000/1000 [==============================] - 1s 689us/step - loss: 0.0031 - acc: 0.0000e+00 - val_loss: 0.0375 - val_acc: 0.0000e+00\n",
      "Epoch 122/150\n",
      "1000/1000 [==============================] - 1s 693us/step - loss: 0.0040 - acc: 0.0000e+00 - val_loss: 0.0459 - val_acc: 0.0000e+00\n",
      "Epoch 123/150\n",
      "1000/1000 [==============================] - 1s 824us/step - loss: 0.0055 - acc: 0.0000e+00 - val_loss: 0.0464 - val_acc: 0.0000e+00\n",
      "Epoch 124/150\n",
      "1000/1000 [==============================] - 1s 742us/step - loss: 0.0043 - acc: 0.0000e+00 - val_loss: 0.0381 - val_acc: 0.0000e+00\n",
      "Epoch 125/150\n",
      "1000/1000 [==============================] - 1s 837us/step - loss: 0.0031 - acc: 0.0000e+00 - val_loss: 0.0344 - val_acc: 0.0000e+00\n",
      "Epoch 126/150\n",
      "1000/1000 [==============================] - 1s 760us/step - loss: 0.0030 - acc: 0.0000e+00 - val_loss: 0.0366 - val_acc: 0.0000e+00\n",
      "Epoch 127/150\n",
      "1000/1000 [==============================] - 1s 830us/step - loss: 0.0042 - acc: 0.0000e+00 - val_loss: 0.0457 - val_acc: 0.0000e+00\n",
      "Epoch 128/150\n",
      "1000/1000 [==============================] - 1s 776us/step - loss: 0.0055 - acc: 0.0000e+00 - val_loss: 0.0466 - val_acc: 0.0000e+00\n",
      "Epoch 129/150\n",
      "1000/1000 [==============================] - 1s 753us/step - loss: 0.0044 - acc: 0.0000e+00 - val_loss: 0.0387 - val_acc: 0.0000e+00\n",
      "Epoch 130/150\n",
      "1000/1000 [==============================] - 1s 778us/step - loss: 0.0033 - acc: 0.0000e+00 - val_loss: 0.0335 - val_acc: 0.0000e+00\n",
      "Epoch 131/150\n",
      "1000/1000 [==============================] - 1s 729us/step - loss: 0.0032 - acc: 0.0000e+00 - val_loss: 0.0373 - val_acc: 0.0000e+00\n",
      "Epoch 132/150\n",
      "1000/1000 [==============================] - 1s 745us/step - loss: 0.0043 - acc: 0.0000e+00 - val_loss: 0.0432 - val_acc: 0.0000e+00\n",
      "Epoch 133/150\n",
      "1000/1000 [==============================] - 1s 778us/step - loss: 0.0040 - acc: 0.0000e+00 - val_loss: 0.0357 - val_acc: 0.0000e+00\n",
      "Epoch 134/150\n",
      "1000/1000 [==============================] - 1s 753us/step - loss: 0.0035 - acc: 0.0000e+00 - val_loss: 0.0398 - val_acc: 0.0000e+00\n",
      "Epoch 135/150\n",
      "1000/1000 [==============================] - 1s 738us/step - loss: 0.0043 - acc: 0.0000e+00 - val_loss: 0.0422 - val_acc: 0.0000e+00\n",
      "Epoch 136/150\n",
      "1000/1000 [==============================] - 1s 761us/step - loss: 0.0047 - acc: 0.0000e+00 - val_loss: 0.0408 - val_acc: 0.0000e+00\n",
      "Epoch 137/150\n",
      "1000/1000 [==============================] - 1s 732us/step - loss: 0.0039 - acc: 0.0000e+00 - val_loss: 0.0347 - val_acc: 0.0000e+00\n",
      "Epoch 138/150\n",
      "1000/1000 [==============================] - 1s 715us/step - loss: 0.0029 - acc: 0.0000e+00 - val_loss: 0.0287 - val_acc: 0.0000e+00\n",
      "Epoch 139/150\n",
      "1000/1000 [==============================] - 1s 688us/step - loss: 0.0024 - acc: 0.0000e+00 - val_loss: 0.0277 - val_acc: 0.0000e+00\n",
      "Epoch 140/150\n",
      "1000/1000 [==============================] - 1s 743us/step - loss: 0.0031 - acc: 0.0000e+00 - val_loss: 0.0398 - val_acc: 0.0000e+00\n",
      "Epoch 141/150\n",
      "1000/1000 [==============================] - 1s 753us/step - loss: 0.0055 - acc: 0.0000e+00 - val_loss: 0.0474 - val_acc: 0.0000e+00\n",
      "Epoch 142/150\n",
      "1000/1000 [==============================] - 1s 761us/step - loss: 0.0054 - acc: 0.0000e+00 - val_loss: 0.0397 - val_acc: 0.0000e+00\n",
      "Epoch 143/150\n",
      "1000/1000 [==============================] - 1s 752us/step - loss: 0.0035 - acc: 0.0000e+00 - val_loss: 0.0304 - val_acc: 0.0000e+00\n",
      "Epoch 144/150\n",
      "1000/1000 [==============================] - 1s 809us/step - loss: 0.0024 - acc: 0.0000e+00 - val_loss: 0.0273 - val_acc: 0.0000e+00\n",
      "Epoch 145/150\n",
      "1000/1000 [==============================] - 1s 794us/step - loss: 0.0024 - acc: 0.0000e+00 - val_loss: 0.0286 - val_acc: 0.0000e+00\n",
      "Epoch 146/150\n",
      "1000/1000 [==============================] - 1s 783us/step - loss: 0.0033 - acc: 0.0000e+00 - val_loss: 0.0412 - val_acc: 0.0000e+00\n",
      "Epoch 147/150\n",
      "1000/1000 [==============================] - 1s 752us/step - loss: 0.0057 - acc: 0.0000e+00 - val_loss: 0.0457 - val_acc: 0.0000e+00\n",
      "Epoch 148/150\n",
      "1000/1000 [==============================] - 1s 776us/step - loss: 0.0044 - acc: 0.0000e+00 - val_loss: 0.0333 - val_acc: 0.0000e+00\n",
      "Epoch 149/150\n",
      "1000/1000 [==============================] - 1s 780us/step - loss: 0.0025 - acc: 0.0000e+00 - val_loss: 0.0253 - val_acc: 0.0000e+00\n",
      "Epoch 150/150\n",
      "1000/1000 [==============================] - 1s 751us/step - loss: 0.0023 - acc: 0.0000e+00 - val_loss: 0.0270 - val_acc: 0.0000e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a2823e668>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=512,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.1,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Result on training set and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_score(model, X_train, y_train, X_test, y_test):\n",
    "    trainScore = model.evaluate(X_train, y_train, verbose=0)\n",
    "    print('Train Score: %.5f MSE (%.2f RMSE)' % (trainScore[0], math.sqrt(trainScore[0])))\n",
    "\n",
    "    testScore = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Test Score: %.5f MSE (%.2f RMSE)' % (testScore[0], math.sqrt(testScore[0])))\n",
    "    return trainScore[0], testScore[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Score: 0.00491 MSE (0.07 RMSE)\n",
      "Test Score: 0.07013 MSE (0.26 RMSE)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0049084834097013579, 0.070133029203104783)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_score(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Prediction vs Real results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage_difference(model, X_test, y_test):\n",
    "    percentage_diff=[]\n",
    "\n",
    "    p = model.predict(X_test)\n",
    "    for u in range(len(y_test)): # for each data index in test data\n",
    "        pr = p[u][0] # pr = prediction on day u\n",
    "\n",
    "        percentage_diff.append((pr-y_test[u]/pr)*100)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = percentage_difference(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Plot out prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(stock_name, normalized_value):\n",
    "    start = datetime.datetime(2000, 1, 1)\n",
    "    end = datetime.date.today()\n",
    "    df = web.DataReader(stock_name, \"yahoo\", start, end)\n",
    "    \n",
    "    df = df['Adj Close'].values.reshape(-1,1)\n",
    "    normalized_value = normalized_value.reshape(-1,1)\n",
    "    \n",
    "    #return df.shape, p.shape\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    a = min_max_scaler.fit_transform(df)\n",
    "    new = min_max_scaler.inverse_transform(normalized_value)\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(stock_name, normalized_value_p, normalized_value_y_test):\n",
    "    newp = denormalize(stock_name, normalized_value_p)\n",
    "    newy_test = denormalize(stock_name, normalized_value_y_test)\n",
    "    plt2.plot(newp, color='red', label='Prediction')\n",
    "    plt2.plot(newy_test,color='blue', label='Actual')\n",
    "    plt2.legend(loc='best')\n",
    "    plt2.title('The test result for {}'.format(stock_name))\n",
    "    plt2.xlabel('Days')\n",
    "    plt2.ylabel('Adjusted Close')\n",
    "\n",
    "    figname = optimizer + stock_name + '_epochs' + str(epochs) + '_' + start.strftime(\"%Y%m%d\") + '-' + end.strftime(\"%Y%m%d\") + '.png'\n",
    "    plt2.savefig(figname, format='png', bbox_inches='tight', transparent=True)\n",
    "    plt2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nduOmQBaHAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1a2823e940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_result(stock_name, p, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Save for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('LSTM_Stock_prediction-20170429.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Fine tune model\n",
    "# 11. Function to load data, train model and see score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_name = '^GSPC'\n",
    "seq_len = 22\n",
    "shape = [4, seq_len, 1] # feature, window, output\n",
    "neurons = [128, 128, 32, 1]\n",
    "epochs = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_measure(stock_name, seq_len, d, shape, neurons, epochs):\n",
    "    df = get_stock_data(stock_name)\n",
    "    X_train, y_train, X_test, y_test = load_data(df, seq_len)\n",
    "    model = build_model2(shape, neurons, d)\n",
    "    model.fit(X_train, y_train, batch_size=512, epochs=epochs, validation_split=0.1, verbose=1)\n",
    "    # model.save('LSTM_Stock_prediction-20170429.h5')\n",
    "    trainScore, testScore = model_score(model, X_train, y_train, X_test, y_test)\n",
    "    return trainScore, testScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 12. Fine tune hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12.1 Optimial Dropout value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 22, 128)           68096     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 22, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 203,841\n",
      "Trainable params: 203,841\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1000 samples, validate on 112 samples\n",
      "Epoch 1/300\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1818 - acc: 0.0000e+00 - val_loss: 0.3273 - val_acc: 0.0000e+00\n",
      "Epoch 2/300\n",
      "1000/1000 [==============================] - 1s 699us/step - loss: 0.0570 - acc: 0.0000e+00 - val_loss: 0.1191 - val_acc: 0.0000e+00\n",
      "Epoch 3/300\n",
      "1000/1000 [==============================] - 1s 702us/step - loss: 0.0264 - acc: 0.0000e+00 - val_loss: 0.1713 - val_acc: 0.0000e+00\n",
      "Epoch 4/300\n",
      "1000/1000 [==============================] - 1s 694us/step - loss: 0.0235 - acc: 0.0000e+00 - val_loss: 0.0735 - val_acc: 0.0000e+00\n",
      "Epoch 5/300\n",
      "1000/1000 [==============================] - 1s 702us/step - loss: 0.0063 - acc: 0.0000e+00 - val_loss: 0.0564 - val_acc: 0.0000e+00\n",
      "Epoch 6/300\n",
      "1000/1000 [==============================] - 1s 693us/step - loss: 0.0043 - acc: 0.0000e+00 - val_loss: 0.0472 - val_acc: 0.0000e+00\n",
      "Epoch 7/300\n",
      "1000/1000 [==============================] - 1s 703us/step - loss: 0.0032 - acc: 0.0000e+00 - val_loss: 0.0410 - val_acc: 0.0000e+00\n",
      "Epoch 8/300\n",
      "1000/1000 [==============================] - 1s 711us/step - loss: 0.0041 - acc: 0.0000e+00 - val_loss: 0.0961 - val_acc: 0.0000e+00\n",
      "Epoch 9/300\n",
      "1000/1000 [==============================] - 1s 714us/step - loss: 0.0297 - acc: 0.0000e+00 - val_loss: 0.1122 - val_acc: 0.0000e+00\n",
      "Epoch 10/300\n",
      "1000/1000 [==============================] - 1s 706us/step - loss: 0.0129 - acc: 0.0000e+00 - val_loss: 0.0613 - val_acc: 0.0000e+00\n",
      "Epoch 11/300\n",
      "1000/1000 [==============================] - 1s 708us/step - loss: 0.0042 - acc: 0.0000e+00 - val_loss: 0.0495 - val_acc: 0.0000e+00\n",
      "Epoch 12/300\n",
      "1000/1000 [==============================] - 1s 711us/step - loss: 0.0033 - acc: 0.0000e+00 - val_loss: 0.0440 - val_acc: 0.0000e+00\n",
      "Epoch 13/300\n",
      "1000/1000 [==============================] - 1s 706us/step - loss: 0.0029 - acc: 0.0000e+00 - val_loss: 0.0425 - val_acc: 0.0000e+00\n",
      "Epoch 14/300\n",
      "1000/1000 [==============================] - 1s 895us/step - loss: 0.0048 - acc: 0.0000e+00 - val_loss: 0.0773 - val_acc: 0.0000e+00\n",
      "Epoch 15/300\n",
      "1000/1000 [==============================] - 1s 789us/step - loss: 0.0185 - acc: 0.0000e+00 - val_loss: 0.0912 - val_acc: 0.0000e+00\n",
      "Epoch 16/300\n",
      "1000/1000 [==============================] - 1s 720us/step - loss: 0.0102 - acc: 0.0000e+00 - val_loss: 0.0607 - val_acc: 0.0000e+00\n",
      "Epoch 17/300\n",
      "1000/1000 [==============================] - 1s 712us/step - loss: 0.0044 - acc: 0.0000e+00 - val_loss: 0.0494 - val_acc: 0.0000e+00\n",
      "Epoch 18/300\n",
      "1000/1000 [==============================] - 1s 709us/step - loss: 0.0034 - acc: 0.0000e+00 - val_loss: 0.0456 - val_acc: 0.0000e+00\n",
      "Epoch 19/300\n",
      "1000/1000 [==============================] - 1s 719us/step - loss: 0.0039 - acc: 0.0000e+00 - val_loss: 0.0567 - val_acc: 0.0000e+00\n",
      "Epoch 20/300\n",
      "1000/1000 [==============================] - 1s 718us/step - loss: 0.0105 - acc: 0.0000e+00 - val_loss: 0.0863 - val_acc: 0.0000e+00\n",
      "Epoch 21/300\n",
      "1000/1000 [==============================] - 1s 698us/step - loss: 0.0126 - acc: 0.0000e+00 - val_loss: 0.0700 - val_acc: 0.0000e+00\n",
      "Epoch 22/300\n",
      "1000/1000 [==============================] - 1s 723us/step - loss: 0.0054 - acc: 0.0000e+00 - val_loss: 0.0538 - val_acc: 0.0000e+00\n",
      "Epoch 23/300\n",
      "1000/1000 [==============================] - 1s 745us/step - loss: 0.0036 - acc: 0.0000e+00 - val_loss: 0.0484 - val_acc: 0.0000e+00\n",
      "Epoch 24/300\n",
      "1000/1000 [==============================] - 1s 732us/step - loss: 0.0039 - acc: 0.0000e+00 - val_loss: 0.0544 - val_acc: 0.0000e+00\n",
      "Epoch 25/300\n",
      "1000/1000 [==============================] - 1s 706us/step - loss: 0.0081 - acc: 0.0000e+00 - val_loss: 0.0827 - val_acc: 0.0000e+00\n",
      "Epoch 26/300\n",
      "1000/1000 [==============================] - 1s 727us/step - loss: 0.0124 - acc: 0.0000e+00 - val_loss: 0.0734 - val_acc: 0.0000e+00\n",
      "Epoch 27/300\n",
      "1000/1000 [==============================] - 1s 760us/step - loss: 0.0062 - acc: 0.0000e+00 - val_loss: 0.0549 - val_acc: 0.0000e+00\n",
      "Epoch 28/300\n",
      "1000/1000 [==============================] - 1s 786us/step - loss: 0.0035 - acc: 0.0000e+00 - val_loss: 0.0459 - val_acc: 0.0000e+00\n",
      "Epoch 29/300\n",
      "1000/1000 [==============================] - 1s 844us/step - loss: 0.0036 - acc: 0.0000e+00 - val_loss: 0.0531 - val_acc: 0.0000e+00\n",
      "Epoch 30/300\n",
      "1000/1000 [==============================] - 1s 757us/step - loss: 0.0082 - acc: 0.0000e+00 - val_loss: 0.0794 - val_acc: 0.0000e+00\n",
      "Epoch 31/300\n",
      "1000/1000 [==============================] - 1s 753us/step - loss: 0.0114 - acc: 0.0000e+00 - val_loss: 0.0708 - val_acc: 0.0000e+00\n",
      "Epoch 32/300\n",
      "1000/1000 [==============================] - 1s 798us/step - loss: 0.0056 - acc: 0.0000e+00 - val_loss: 0.0517 - val_acc: 0.0000e+00\n",
      "Epoch 33/300\n",
      "1000/1000 [==============================] - 1s 873us/step - loss: 0.0031 - acc: 0.0000e+00 - val_loss: 0.0420 - val_acc: 0.0000e+00\n",
      "Epoch 34/300\n",
      "1000/1000 [==============================] - 1s 728us/step - loss: 0.0027 - acc: 0.0000e+00 - val_loss: 0.0430 - val_acc: 0.0000e+00\n",
      "Epoch 35/300\n",
      "1000/1000 [==============================] - 1s 785us/step - loss: 0.0054 - acc: 0.0000e+00 - val_loss: 0.0774 - val_acc: 0.0000e+00\n",
      "Epoch 36/300\n",
      "1000/1000 [==============================] - 1s 766us/step - loss: 0.0176 - acc: 0.0000e+00 - val_loss: 0.0866 - val_acc: 0.0000e+00\n",
      "Epoch 37/300\n",
      "1000/1000 [==============================] - 1s 765us/step - loss: 0.0071 - acc: 0.0000e+00 - val_loss: 0.0503 - val_acc: 0.0000e+00\n",
      "Epoch 38/300\n",
      "1000/1000 [==============================] - 1s 793us/step - loss: 0.0032 - acc: 0.0000e+00 - val_loss: 0.0424 - val_acc: 0.0000e+00\n",
      "Epoch 39/300\n",
      "1000/1000 [==============================] - 1s 787us/step - loss: 0.0028 - acc: 0.0000e+00 - val_loss: 0.0392 - val_acc: 0.0000e+00\n",
      "Epoch 40/300\n",
      "1000/1000 [==============================] - 1s 835us/step - loss: 0.0030 - acc: 0.0000e+00 - val_loss: 0.0471 - val_acc: 0.0000e+00\n",
      "Epoch 41/300\n",
      "1000/1000 [==============================] - 1s 836us/step - loss: 0.0084 - acc: 0.0000e+00 - val_loss: 0.0802 - val_acc: 0.0000e+00\n",
      "Epoch 42/300\n",
      "1000/1000 [==============================] - 1s 816us/step - loss: 0.0113 - acc: 0.0000e+00 - val_loss: 0.0653 - val_acc: 0.0000e+00\n",
      "Epoch 43/300\n",
      "1000/1000 [==============================] - 1s 773us/step - loss: 0.0045 - acc: 0.0000e+00 - val_loss: 0.0447 - val_acc: 0.0000e+00\n",
      "Epoch 44/300\n",
      "1000/1000 [==============================] - 1s 808us/step - loss: 0.0029 - acc: 0.0000e+00 - val_loss: 0.0359 - val_acc: 0.0000e+00\n",
      "Epoch 45/300\n",
      "1000/1000 [==============================] - 1s 832us/step - loss: 0.0025 - acc: 0.0000e+00 - val_loss: 0.0316 - val_acc: 0.0000e+00\n",
      "Epoch 46/300\n",
      "1000/1000 [==============================] - 1s 826us/step - loss: 0.0023 - acc: 0.0000e+00 - val_loss: 0.0363 - val_acc: 0.0000e+00\n",
      "Epoch 47/300\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.0071 - acc: 0.0000e+00 - val_loss: 0.0908 - val_acc: 0.0000e+00\n",
      "Epoch 48/300\n",
      "1000/1000 [==============================] - 1s 911us/step - loss: 0.0161 - acc: 0.0000e+00 - val_loss: 0.0706 - val_acc: 0.0000e+00\n",
      "Epoch 49/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 1s 994us/step - loss: 0.0055 - acc: 0.0000e+00 - val_loss: 0.0506 - val_acc: 0.0000e+00\n",
      "Epoch 50/300\n",
      "1000/1000 [==============================] - 1s 914us/step - loss: 0.0032 - acc: 0.0000e+00 - val_loss: 0.0428 - val_acc: 0.0000e+00\n",
      "Epoch 51/300\n",
      "1000/1000 [==============================] - 1s 915us/step - loss: 0.0030 - acc: 0.0000e+00 - val_loss: 0.0457 - val_acc: 0.0000e+00\n",
      "Epoch 52/300\n",
      "1000/1000 [==============================] - 1s 860us/step - loss: 0.0051 - acc: 0.0000e+00 - val_loss: 0.0657 - val_acc: 0.0000e+00\n",
      "Epoch 53/300\n",
      "1000/1000 [==============================] - 1s 865us/step - loss: 0.0100 - acc: 0.0000e+00 - val_loss: 0.0679 - val_acc: 0.0000e+00\n",
      "Epoch 54/300\n",
      "1000/1000 [==============================] - 1s 840us/step - loss: 0.0068 - acc: 0.0000e+00 - val_loss: 0.0564 - val_acc: 0.0000e+00\n",
      "Epoch 55/300\n",
      "1000/1000 [==============================] - 1s 867us/step - loss: 0.0041 - acc: 0.0000e+00 - val_loss: 0.0473 - val_acc: 0.0000e+00\n",
      "Epoch 56/300\n",
      "1000/1000 [==============================] - 1s 868us/step - loss: 0.0037 - acc: 0.0000e+00 - val_loss: 0.0481 - val_acc: 0.0000e+00\n",
      "Epoch 57/300\n",
      "1000/1000 [==============================] - 1s 858us/step - loss: 0.0053 - acc: 0.0000e+00 - val_loss: 0.0617 - val_acc: 0.0000e+00\n",
      "Epoch 58/300\n",
      "1000/1000 [==============================] - 1s 969us/step - loss: 0.0085 - acc: 0.0000e+00 - val_loss: 0.0677 - val_acc: 0.0000e+00\n",
      "Epoch 59/300\n",
      "1000/1000 [==============================] - 1s 976us/step - loss: 0.0078 - acc: 0.0000e+00 - val_loss: 0.0593 - val_acc: 0.0000e+00\n",
      "Epoch 60/300\n",
      "1000/1000 [==============================] - 1s 857us/step - loss: 0.0044 - acc: 0.0000e+00 - val_loss: 0.0475 - val_acc: 0.0000e+00\n",
      "Epoch 61/300\n",
      "1000/1000 [==============================] - 1s 922us/step - loss: 0.0036 - acc: 0.0000e+00 - val_loss: 0.0463 - val_acc: 0.0000e+00\n",
      "Epoch 62/300\n",
      "1000/1000 [==============================] - 1s 864us/step - loss: 0.0048 - acc: 0.0000e+00 - val_loss: 0.0564 - val_acc: 0.0000e+00\n",
      "Epoch 63/300\n",
      "1000/1000 [==============================] - 1s 867us/step - loss: 0.0081 - acc: 0.0000e+00 - val_loss: 0.0670 - val_acc: 0.0000e+00\n",
      "Epoch 64/300\n",
      "1000/1000 [==============================] - 1s 821us/step - loss: 0.0073 - acc: 0.0000e+00 - val_loss: 0.0545 - val_acc: 0.0000e+00\n",
      "Epoch 65/300\n",
      "1000/1000 [==============================] - 1s 975us/step - loss: 0.0045 - acc: 0.0000e+00 - val_loss: 0.0480 - val_acc: 0.0000e+00\n",
      "Epoch 66/300\n",
      "1000/1000 [==============================] - 1s 822us/step - loss: 0.0036 - acc: 0.0000e+00 - val_loss: 0.0446 - val_acc: 0.0000e+00\n",
      "Epoch 67/300\n",
      "1000/1000 [==============================] - 1s 862us/step - loss: 0.0039 - acc: 0.0000e+00 - val_loss: 0.0522 - val_acc: 0.0000e+00\n",
      "Epoch 68/300\n",
      "1000/1000 [==============================] - 1s 921us/step - loss: 0.0067 - acc: 0.0000e+00 - val_loss: 0.0635 - val_acc: 0.0000e+00\n",
      "Epoch 69/300\n",
      "1000/1000 [==============================] - 1s 812us/step - loss: 0.0068 - acc: 0.0000e+00 - val_loss: 0.0526 - val_acc: 0.0000e+00\n",
      "Epoch 70/300\n",
      "1000/1000 [==============================] - 1s 901us/step - loss: 0.0041 - acc: 0.0000e+00 - val_loss: 0.0464 - val_acc: 0.0000e+00\n",
      "Epoch 71/300\n",
      "1000/1000 [==============================] - 1s 811us/step - loss: 0.0037 - acc: 0.0000e+00 - val_loss: 0.0512 - val_acc: 0.0000e+00\n",
      "Epoch 72/300\n",
      "1000/1000 [==============================] - 1s 777us/step - loss: 0.0059 - acc: 0.0000e+00 - val_loss: 0.0605 - val_acc: 0.0000e+00\n",
      "Epoch 73/300\n",
      "1000/1000 [==============================] - 1s 760us/step - loss: 0.0064 - acc: 0.0000e+00 - val_loss: 0.0535 - val_acc: 0.0000e+00\n",
      "Epoch 74/300\n",
      "1000/1000 [==============================] - 1s 766us/step - loss: 0.0049 - acc: 0.0000e+00 - val_loss: 0.0503 - val_acc: 0.0000e+00\n",
      "Epoch 75/300\n",
      "1000/1000 [==============================] - 1s 745us/step - loss: 0.0045 - acc: 0.0000e+00 - val_loss: 0.0490 - val_acc: 0.0000e+00\n",
      "Epoch 76/300\n",
      "1000/1000 [==============================] - 1s 714us/step - loss: 0.0050 - acc: 0.0000e+00 - val_loss: 0.0530 - val_acc: 0.0000e+00\n",
      "Epoch 77/300\n",
      "1000/1000 [==============================] - 1s 715us/step - loss: 0.0056 - acc: 0.0000e+00 - val_loss: 0.0549 - val_acc: 0.0000e+00\n",
      "Epoch 78/300\n",
      "1000/1000 [==============================] - 1s 826us/step - loss: 0.0055 - acc: 0.0000e+00 - val_loss: 0.0524 - val_acc: 0.0000e+00\n",
      "Epoch 79/300\n",
      "1000/1000 [==============================] - 1s 735us/step - loss: 0.0055 - acc: 0.0000e+00 - val_loss: 0.0537 - val_acc: 0.0000e+00\n",
      "Epoch 80/300\n",
      "1000/1000 [==============================] - 1s 747us/step - loss: 0.0052 - acc: 0.0000e+00 - val_loss: 0.0537 - val_acc: 0.0000e+00\n",
      "Epoch 81/300\n",
      "1000/1000 [==============================] - 1s 713us/step - loss: 0.0050 - acc: 0.0000e+00 - val_loss: 0.0510 - val_acc: 0.0000e+00\n",
      "Epoch 82/300\n",
      "1000/1000 [==============================] - 1s 763us/step - loss: 0.0038 - acc: 0.0000e+00 - val_loss: 0.0432 - val_acc: 0.0000e+00\n",
      "Epoch 83/300\n",
      "1000/1000 [==============================] - 1s 779us/step - loss: 0.0034 - acc: 0.0000e+00 - val_loss: 0.0447 - val_acc: 0.0000e+00\n",
      "Epoch 84/300\n",
      "1000/1000 [==============================] - 1s 744us/step - loss: 0.0049 - acc: 0.0000e+00 - val_loss: 0.0575 - val_acc: 0.0000e+00\n",
      "Epoch 85/300\n",
      "1000/1000 [==============================] - 1s 763us/step - loss: 0.0067 - acc: 0.0000e+00 - val_loss: 0.0562 - val_acc: 0.0000e+00\n",
      "Epoch 86/300\n",
      "1000/1000 [==============================] - 1s 791us/step - loss: 0.0061 - acc: 0.0000e+00 - val_loss: 0.0576 - val_acc: 0.0000e+00\n",
      "Epoch 87/300\n",
      "1000/1000 [==============================] - 1s 788us/step - loss: 0.0058 - acc: 0.0000e+00 - val_loss: 0.0517 - val_acc: 0.0000e+00\n",
      "Epoch 88/300\n",
      "1000/1000 [==============================] - 1s 763us/step - loss: 0.0040 - acc: 0.0000e+00 - val_loss: 0.0421 - val_acc: 0.0000e+00\n",
      "Epoch 89/300\n",
      "1000/1000 [==============================] - 1s 756us/step - loss: 0.0029 - acc: 0.0000e+00 - val_loss: 0.0393 - val_acc: 0.0000e+00\n",
      "Epoch 90/300\n",
      "1000/1000 [==============================] - 1s 743us/step - loss: 0.0031 - acc: 0.0000e+00 - val_loss: 0.0428 - val_acc: 0.0000e+00\n",
      "Epoch 91/300\n",
      "1000/1000 [==============================] - 1s 725us/step - loss: 0.0053 - acc: 0.0000e+00 - val_loss: 0.0590 - val_acc: 0.0000e+00\n",
      "Epoch 92/300\n",
      "1000/1000 [==============================] - 1s 724us/step - loss: 0.0075 - acc: 0.0000e+00 - val_loss: 0.0562 - val_acc: 0.0000e+00\n",
      "Epoch 93/300\n",
      "1000/1000 [==============================] - 1s 737us/step - loss: 0.0050 - acc: 0.0000e+00 - val_loss: 0.0463 - val_acc: 0.0000e+00\n",
      "Epoch 94/300\n",
      "1000/1000 [==============================] - 1s 783us/step - loss: 0.0033 - acc: 0.0000e+00 - val_loss: 0.0382 - val_acc: 0.0000e+00\n",
      "Epoch 95/300\n",
      "1000/1000 [==============================] - 1s 719us/step - loss: 0.0032 - acc: 0.0000e+00 - val_loss: 0.0419 - val_acc: 0.0000e+00\n",
      "Epoch 96/300\n",
      "1000/1000 [==============================] - 1s 789us/step - loss: 0.0048 - acc: 0.0000e+00 - val_loss: 0.0545 - val_acc: 0.0000e+00\n",
      "Epoch 97/300\n",
      "1000/1000 [==============================] - 1s 798us/step - loss: 0.0073 - acc: 0.0000e+00 - val_loss: 0.0566 - val_acc: 0.0000e+00\n",
      "Epoch 98/300\n",
      "1000/1000 [==============================] - 1s 775us/step - loss: 0.0055 - acc: 0.0000e+00 - val_loss: 0.0461 - val_acc: 0.0000e+00\n",
      "Epoch 99/300\n",
      "1000/1000 [==============================] - 1s 778us/step - loss: 0.0033 - acc: 0.0000e+00 - val_loss: 0.0353 - val_acc: 0.0000e+00\n",
      "Epoch 100/300\n",
      "1000/1000 [==============================] - 1s 730us/step - loss: 0.0025 - acc: 0.0000e+00 - val_loss: 0.0344 - val_acc: 0.0000e+00\n",
      "Epoch 101/300\n",
      "1000/1000 [==============================] - 1s 758us/step - loss: 0.0036 - acc: 0.0000e+00 - val_loss: 0.0480 - val_acc: 0.0000e+00\n",
      "Epoch 102/300\n",
      "1000/1000 [==============================] - 1s 774us/step - loss: 0.0073 - acc: 0.0000e+00 - val_loss: 0.0592 - val_acc: 0.0000e+00\n",
      "Epoch 103/300\n",
      "1000/1000 [==============================] - 1s 751us/step - loss: 0.0069 - acc: 0.0000e+00 - val_loss: 0.0493 - val_acc: 0.0000e+00\n",
      "Epoch 104/300\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-53597ff55d8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrainScore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestScore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquick_measure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstock_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneurons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mdropout_result\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestScore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-701561477536>\u001b[0m in \u001b[0;36mquick_measure\u001b[0;34m(stock_name, seq_len, d, shape, neurons, epochs)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneurons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;31m# model.save('LSTM_Stock_prediction-20170429.h5')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtrainScore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestScore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    891\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m~/anaconda2/envs/py3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1629\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/anaconda2/envs/py3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2330\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2331\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2332\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2333\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m~/anaconda2/envs/py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dlist = [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]\n",
    "neurons_LSTM = [32, 64, 128, 256, 512, 1024, 2048]\n",
    "dropout_result = {}\n",
    "\n",
    "for d in dlist:    \n",
    "    trainScore, testScore = quick_measure(stock_name, seq_len, d, shape, neurons, epochs)\n",
    "    dropout_result[d] = testScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_val = min(dropout_result.values())\n",
    "min_val_key = [k for k, v in dropout_result.items() if v == min_val]\n",
    "print (dropout_result)\n",
    "print (min_val_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lists = sorted(dropout_result.items())\n",
    "x,y = zip(*lists)\n",
    "plt.plot(x,y)\n",
    "plt.title('Finding the best hyperparameter')\n",
    "plt.xlabel('Dropout')\n",
    "plt.ylabel('Mean Square Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12.2 Optimial epochs value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_name = '^GSPC'\n",
    "seq_len = 22\n",
    "shape = [4, seq_len, 1] # feature, window, output\n",
    "neurons = [128, 128, 32, 1]\n",
    "epochslist = [10,20,30,40,50,60,70,80,90,100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_result = {}\n",
    "\n",
    "for epochs in epochslist:    \n",
    "    trainScore, testScore = quick_measure(stock_name, seq_len, d, shape, neurons, epochs)\n",
    "    epochs_result[epochs] = testScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lists = sorted(epochs_result.items())\n",
    "x,y = zip(*lists)\n",
    "plt.plot(x,y)\n",
    "plt.title('Finding the best hyperparameter')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Mean Square Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12.3 Optimal number of neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_name = '^GSPC'\n",
    "seq_len = 22\n",
    "shape = [4, seq_len, 1] # feature, window, output\n",
    "epochs = 90\n",
    "dropout = 0.3\n",
    "neuronlist1 = [32, 64, 128, 256, 512]\n",
    "neuronlist2 = [16, 32, 64]\n",
    "neurons_result = {}\n",
    "\n",
    "for neuron_lstm in neuronlist1:\n",
    "    neurons = [neuron_lstm, neuron_lstm]\n",
    "    for activation in neuronlist2:\n",
    "        neurons.append(activation)\n",
    "        neurons.append(1)\n",
    "        trainScore, testScore = quick_measure(stock_name, seq_len, d, shape, neurons, epochs)\n",
    "        neurons_result[str(neurons)] = testScore\n",
    "        neurons = neurons[:2]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lists = sorted(neurons_result.items())\n",
    "x,y = zip(*lists)\n",
    "\n",
    "plt.title('Finding the best hyperparameter')\n",
    "plt.xlabel('neurons')\n",
    "plt.ylabel('Mean Square Error')\n",
    "\n",
    "plt.bar(range(len(lists)), y, align='center')\n",
    "plt.xticks(range(len(lists)), x)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "12.4 Optimial Dropout value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_name = '^GSPC'\n",
    "seq_len = 22\n",
    "shape = [4, seq_len, 1] # feature, window, output\n",
    "neurons = [256, 256, 32, 1]\n",
    "epochs = 90\n",
    "decaylist = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model3(layers, neurons, d, decay):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(neurons[0], input_shape=(layers[1], layers[0]), return_sequences=True))\n",
    "    model.add(Dropout(d))\n",
    "        \n",
    "    model.add(LSTM(neurons[1], input_shape=(layers[1], layers[0]), return_sequences=False))\n",
    "    model.add(Dropout(d))\n",
    "        \n",
    "    model.add(Dense(neurons[2],kernel_initializer=\"uniform\",activation='relu'))        \n",
    "    model.add(Dense(neurons[3],kernel_initializer=\"uniform\",activation='linear'))\n",
    "    # model = load_model('my_LSTM_stock_model1000.h5')\n",
    "    adam = keras.optimizers.Adam(decay=decay)\n",
    "    model.compile(loss='mse',optimizer='adam', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_measure(stock_name, seq_len, d, shape, neurons, epochs, decay):\n",
    "    df = get_stock_data(stock_name)\n",
    "    X_train, y_train, X_test, y_test = load_data(df, seq_len)\n",
    "    model = build_model3(shape, neurons, d, decay)\n",
    "    model.fit(X_train, y_train, batch_size=512, epochs=epochs, validation_split=0.1, verbose=1)\n",
    "    # model.save('LSTM_Stock_prediction-20170429.h5')\n",
    "    trainScore, testScore = model_score(model, X_train, y_train, X_test, y_test)\n",
    "    return trainScore, testScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decay_result = {}\n",
    "\n",
    "for decay in decaylist:    \n",
    "    trainScore, testScore = quick_measure(stock_name, seq_len, d, shape, neurons, epochs, decay)\n",
    "    decay_result[decay] = testScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lists = sorted(decay_result.items())\n",
    "x,y = zip(*lists)\n",
    "plt.plot(x,y)\n",
    "plt.title('Finding the best hyperparameter')\n",
    "plt.xlabel('Decay')\n",
    "plt.ylabel('Mean Square Error')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_name = '^GSPC'\n",
    "neurons = [256, 256, 32, 1]\n",
    "epochs = 90\n",
    "d = 0.3 #dropout\n",
    "decay = 0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len_list = [5, 10, 22, 60, 120, 180]\n",
    "\n",
    "seq_len_result = {}\n",
    "\n",
    "for seq_len in seq_len_list:\n",
    "    shape = [4, seq_len, 1]\n",
    "    \n",
    "    trainScore, testScore = quick_measure(stock_name, seq_len, d, shape, neurons, epochs, decay)\n",
    "    seq_len_result[seq_len] = testScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lists = sorted(seq_len_result.items())\n",
    "x,y = zip(*lists)\n",
    "plt.plot(x,y)\n",
    "plt.title('Finding the best hyperparameter')\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Mean Square Error')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
