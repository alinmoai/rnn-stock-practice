{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stock value prediction from Open, High, Low"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Import module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt2\n",
    "import pandas as pd\n",
    "from pandas import datetime\n",
    "import math, time\n",
    "import itertools\n",
    "from sklearn import preprocessing\n",
    "import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import load_model\n",
    "import keras\n",
    "import pandas_datareader.data as web\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_name = '^GSPC'\n",
    "seq_len = 22\n",
    "d = 0.2\n",
    "shape = [4, seq_len, 1] # feature, window, output\n",
    "neurons = [128, 128, 32, 1]\n",
    "epochs = 100\n",
    "\n",
    "start = datetime.datetime(2013, 1, 1)\n",
    "end = datetime.date.today()\n",
    "\n",
    "optimizer = 'RMSprop'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 1. Download data and normalize it\n",
    "Data since 1950 to today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_data(stock_name, normalize=True):\n",
    "    \n",
    "    df = web.DataReader(stock_name, \"yahoo\", start, end)\n",
    "    df.drop(['Volume', 'Close'], 1, inplace=True)\n",
    "    \n",
    "    if normalize:\n",
    "#         yesterday_value = pd.concat([df[:1], df[:len(df)-1]], ignore_index=True)\n",
    "#         df = pd.concat([df[:]], ignore_index=True)\n",
    "#         df = df / yesterday_value\n",
    "        \n",
    "        min_max_scaler = preprocessing.MinMaxScaler()\n",
    "        df['Open'] = min_max_scaler.fit_transform(df.Open.values.reshape(-1,1))\n",
    "        df['High'] = min_max_scaler.fit_transform(df.High.values.reshape(-1,1))\n",
    "        df['Low'] = min_max_scaler.fit_transform(df.Low.values.reshape(-1,1))\n",
    "        df['Adj Close'] = min_max_scaler.fit_transform(df['Adj Close'].values.reshape(-1,1))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_stock_data(stock_name, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Plot out the Normalized Adjusted close price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stock(stock_name):\n",
    "    df = get_stock_data(stock_name, normalize=True)\n",
    "    print(df.head())\n",
    "    plt.plot(df['Adj Close'], color='red', label='Adj Close')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                Open      High       Low  Adj Close\n",
      "Date                                               \n",
      "2012-12-31  0.000000  0.000000  0.000000   0.000000\n",
      "2013-01-02  0.018415  0.028142  0.021804   0.028664\n",
      "2013-01-03  0.046494  0.030539  0.044587   0.026251\n",
      "2013-01-04  0.044130  0.032486  0.047274   0.031868\n",
      "2013-01-07  0.049633  0.031327  0.045434   0.028244\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXsAAAEJCAYAAAB11IfBAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1057a0f28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_stock(stock_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Set last day Adjusted Close as y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(stock, seq_len):\n",
    "    amount_of_features = len(stock.columns)\n",
    "    data = stock.as_matrix() \n",
    "    sequence_length = seq_len + 1 # index starting from 0\n",
    "    result = []\n",
    "    \n",
    "    for index in range(len(data) - sequence_length): # maxmimum date = lastest date - sequence length\n",
    "        result.append(data[index: index + sequence_length]) # index : index + 22days\n",
    "    \n",
    "    result = np.array(result)\n",
    "    row = round(0.9 * result.shape[0]) # 90% split\n",
    "    \n",
    "    train = result[:int(row), :] # 90% date\n",
    "    X_train = train[:, :-1] # all data until day m\n",
    "    y_train = train[:, -1][:,-1] # day m + 1 adjusted close price\n",
    "    \n",
    "    X_test = result[int(row):, :-1]\n",
    "    y_test = result[int(row):, -1][:,-1] \n",
    "\n",
    "    X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], amount_of_features))\n",
    "    X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], amount_of_features))  \n",
    "\n",
    "    return [X_train, y_train, X_test, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = load_data(df, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1112, 22, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[0], X_train.shape[1], X_train.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1112"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 4. Buidling neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model2(layers, neurons, d):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(neurons[0], input_shape=(layers[1], layers[0]), return_sequences=True))\n",
    "    model.add(Dropout(d))\n",
    "        \n",
    "    model.add(LSTM(neurons[1], input_shape=(layers[1], layers[0]), return_sequences=False))\n",
    "    model.add(Dropout(d))\n",
    "        \n",
    "    model.add(Dense(neurons[2],kernel_initializer=\"uniform\",activation='relu'))        \n",
    "    model.add(Dense(neurons[3],kernel_initializer=\"uniform\",activation='linear'))\n",
    "    # model = load_model('my_LSTM_stock_model1000.h5')\n",
    "    # adam = keras.optimizers.Adam(decay=0.2)\n",
    "    model.compile(loss='mse',optimizer=optimizer , metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Model Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 22, 128)           68096     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 22, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 203,841\n",
      "Trainable params: 203,841\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model2(shape, neurons, d)\n",
    "# layers = [4, 22, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples, validate on 112 samples\n",
      "Epoch 1/100\n",
      "1000/1000 [==============================] - 1s 1ms/step - loss: 0.1832 - acc: 0.0000e+00 - val_loss: 0.3532 - val_acc: 0.0000e+00\n",
      "Epoch 2/100\n",
      "1000/1000 [==============================] - 1s 685us/step - loss: 0.0619 - acc: 0.0000e+00 - val_loss: 0.0461 - val_acc: 0.0000e+00\n",
      "Epoch 3/100\n",
      "1000/1000 [==============================] - 1s 719us/step - loss: 0.0087 - acc: 0.0000e+00 - val_loss: 0.0043 - val_acc: 0.0000e+00\n",
      "Epoch 4/100\n",
      "1000/1000 [==============================] - 1s 719us/step - loss: 0.0696 - acc: 0.0000e+00 - val_loss: 0.0633 - val_acc: 0.0000e+00\n",
      "Epoch 5/100\n",
      "1000/1000 [==============================] - 1s 734us/step - loss: 0.0059 - acc: 0.0000e+00 - val_loss: 0.0591 - val_acc: 0.0000e+00\n",
      "Epoch 6/100\n",
      "1000/1000 [==============================] - 1s 698us/step - loss: 0.0047 - acc: 0.0000e+00 - val_loss: 0.0506 - val_acc: 0.0000e+00\n",
      "Epoch 7/100\n",
      "1000/1000 [==============================] - 1s 707us/step - loss: 0.0037 - acc: 0.0000e+00 - val_loss: 0.0447 - val_acc: 0.0000e+00\n",
      "Epoch 8/100\n",
      "1000/1000 [==============================] - 1s 723us/step - loss: 0.0029 - acc: 0.0000e+00 - val_loss: 0.0387 - val_acc: 0.0000e+00\n",
      "Epoch 9/100\n",
      "1000/1000 [==============================] - 1s 716us/step - loss: 0.0027 - acc: 0.0000e+00 - val_loss: 0.0459 - val_acc: 0.0000e+00\n",
      "Epoch 10/100\n",
      "1000/1000 [==============================] - 1s 715us/step - loss: 0.0103 - acc: 0.0000e+00 - val_loss: 0.1053 - val_acc: 0.0000e+00\n",
      "Epoch 11/100\n",
      "1000/1000 [==============================] - 1s 733us/step - loss: 0.0217 - acc: 0.0000e+00 - val_loss: 0.0821 - val_acc: 0.0000e+00\n",
      "Epoch 12/100\n",
      "1000/1000 [==============================] - 1s 780us/step - loss: 0.0068 - acc: 0.0000e+00 - val_loss: 0.0571 - val_acc: 0.0000e+00\n",
      "Epoch 13/100\n",
      "1000/1000 [==============================] - 1s 719us/step - loss: 0.0037 - acc: 0.0000e+00 - val_loss: 0.0470 - val_acc: 0.0000e+00\n",
      "Epoch 14/100\n",
      "1000/1000 [==============================] - 1s 723us/step - loss: 0.0029 - acc: 0.0000e+00 - val_loss: 0.0451 - val_acc: 0.0000e+00\n",
      "Epoch 15/100\n",
      "1000/1000 [==============================] - 1s 736us/step - loss: 0.0046 - acc: 0.0000e+00 - val_loss: 0.0714 - val_acc: 0.0000e+00\n",
      "Epoch 16/100\n",
      "1000/1000 [==============================] - 1s 717us/step - loss: 0.0179 - acc: 0.0000e+00 - val_loss: 0.1015 - val_acc: 0.0000e+00\n",
      "Epoch 17/100\n",
      "1000/1000 [==============================] - 1s 716us/step - loss: 0.0128 - acc: 0.0000e+00 - val_loss: 0.0651 - val_acc: 0.0000e+00\n",
      "Epoch 18/100\n",
      "1000/1000 [==============================] - 1s 727us/step - loss: 0.0042 - acc: 0.0000e+00 - val_loss: 0.0528 - val_acc: 0.0000e+00\n",
      "Epoch 19/100\n",
      "1000/1000 [==============================] - 1s 720us/step - loss: 0.0034 - acc: 0.0000e+00 - val_loss: 0.0483 - val_acc: 0.0000e+00\n",
      "Epoch 20/100\n",
      "1000/1000 [==============================] - 1s 701us/step - loss: 0.0040 - acc: 0.0000e+00 - val_loss: 0.0583 - val_acc: 0.0000e+00\n",
      "Epoch 21/100\n",
      "1000/1000 [==============================] - 1s 707us/step - loss: 0.0115 - acc: 0.0000e+00 - val_loss: 0.0937 - val_acc: 0.0000e+00\n",
      "Epoch 22/100\n",
      "1000/1000 [==============================] - 1s 727us/step - loss: 0.0131 - acc: 0.0000e+00 - val_loss: 0.0708 - val_acc: 0.0000e+00\n",
      "Epoch 23/100\n",
      "1000/1000 [==============================] - 1s 713us/step - loss: 0.0050 - acc: 0.0000e+00 - val_loss: 0.0533 - val_acc: 0.0000e+00\n",
      "Epoch 24/100\n",
      "1000/1000 [==============================] - 1s 712us/step - loss: 0.0034 - acc: 0.0000e+00 - val_loss: 0.0433 - val_acc: 0.0000e+00\n",
      "Epoch 25/100\n",
      "1000/1000 [==============================] - 1s 710us/step - loss: 0.0031 - acc: 0.0000e+00 - val_loss: 0.0527 - val_acc: 0.0000e+00\n",
      "Epoch 26/100\n",
      "1000/1000 [==============================] - 1s 718us/step - loss: 0.0100 - acc: 0.0000e+00 - val_loss: 0.0955 - val_acc: 0.0000e+00\n",
      "Epoch 27/100\n",
      "1000/1000 [==============================] - 1s 723us/step - loss: 0.0165 - acc: 0.0000e+00 - val_loss: 0.0820 - val_acc: 0.0000e+00\n",
      "Epoch 28/100\n",
      "1000/1000 [==============================] - 1s 719us/step - loss: 0.0065 - acc: 0.0000e+00 - val_loss: 0.0567 - val_acc: 0.0000e+00\n",
      "Epoch 29/100\n",
      "1000/1000 [==============================] - 1s 703us/step - loss: 0.0036 - acc: 0.0000e+00 - val_loss: 0.0471 - val_acc: 0.0000e+00\n",
      "Epoch 30/100\n",
      "1000/1000 [==============================] - 1s 695us/step - loss: 0.0034 - acc: 0.0000e+00 - val_loss: 0.0535 - val_acc: 0.0000e+00\n",
      "Epoch 31/100\n",
      "1000/1000 [==============================] - 1s 695us/step - loss: 0.0085 - acc: 0.0000e+00 - val_loss: 0.0863 - val_acc: 0.0000e+00\n",
      "Epoch 32/100\n",
      "1000/1000 [==============================] - 1s 690us/step - loss: 0.0123 - acc: 0.0000e+00 - val_loss: 0.0744 - val_acc: 0.0000e+00\n",
      "Epoch 33/100\n",
      "1000/1000 [==============================] - 1s 704us/step - loss: 0.0056 - acc: 0.0000e+00 - val_loss: 0.0570 - val_acc: 0.0000e+00\n",
      "Epoch 34/100\n",
      "1000/1000 [==============================] - 1s 699us/step - loss: 0.0045 - acc: 0.0000e+00 - val_loss: 0.0568 - val_acc: 0.0000e+00\n",
      "Epoch 35/100\n",
      "1000/1000 [==============================] - 1s 709us/step - loss: 0.0057 - acc: 0.0000e+00 - val_loss: 0.0701 - val_acc: 0.0000e+00\n",
      "Epoch 36/100\n",
      "1000/1000 [==============================] - 1s 715us/step - loss: 0.0099 - acc: 0.0000e+00 - val_loss: 0.0771 - val_acc: 0.0000e+00\n",
      "Epoch 37/100\n",
      "1000/1000 [==============================] - 1s 711us/step - loss: 0.0071 - acc: 0.0000e+00 - val_loss: 0.0610 - val_acc: 0.0000e+00\n",
      "Epoch 38/100\n",
      "1000/1000 [==============================] - 1s 680us/step - loss: 0.0048 - acc: 0.0000e+00 - val_loss: 0.0561 - val_acc: 0.0000e+00\n",
      "Epoch 39/100\n",
      "1000/1000 [==============================] - 1s 694us/step - loss: 0.0052 - acc: 0.0000e+00 - val_loss: 0.0631 - val_acc: 0.0000e+00\n",
      "Epoch 40/100\n",
      "1000/1000 [==============================] - 1s 722us/step - loss: 0.0084 - acc: 0.0000e+00 - val_loss: 0.0758 - val_acc: 0.0000e+00\n",
      "Epoch 41/100\n",
      " 512/1000 [==============>...............] - ETA: 0s - loss: 0.0091 - acc: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=512,\n",
    "    epochs=epochs,\n",
    "    validation_split=0.1,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Result on training set and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_score(model, X_train, y_train, X_test, y_test):\n",
    "    trainScore = model.evaluate(X_train, y_train, verbose=0)\n",
    "    print('Train Score: %.5f MSE (%.2f RMSE)' % (trainScore[0], math.sqrt(trainScore[0])))\n",
    "\n",
    "    testScore = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print('Test Score: %.5f MSE (%.2f RMSE)' % (testScore[0], math.sqrt(testScore[0])))\n",
    "    return trainScore[0], testScore[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_score(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Prediction vs Real results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percentage_difference(model, X_test, y_test):\n",
    "    percentage_diff=[]\n",
    "\n",
    "    p = model.predict(X_test)\n",
    "    for u in range(len(y_test)): # for each data index in test data\n",
    "        pr = p[u][0] # pr = prediction on day u\n",
    "\n",
    "        percentage_diff.append((pr-y_test[u]/pr)*100)\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = percentage_difference(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Plot out prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(stock_name, normalized_value):\n",
    "    start = datetime.datetime(2000, 1, 1)\n",
    "    end = datetime.date.today()\n",
    "    df = web.DataReader(stock_name, \"yahoo\", start, end)\n",
    "    \n",
    "    df = df['Adj Close'].values.reshape(-1,1)\n",
    "    normalized_value = normalized_value.reshape(-1,1)\n",
    "    \n",
    "    #return df.shape, p.shape\n",
    "    min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    a = min_max_scaler.fit_transform(df)\n",
    "    new = min_max_scaler.inverse_transform(normalized_value)\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_result(stock_name, normalized_value_p, normalized_value_y_test):\n",
    "    newp = denormalize(stock_name, normalized_value_p)\n",
    "    newy_test = denormalize(stock_name, normalized_value_y_test)\n",
    "    plt2.plot(newp, color='red', label='Prediction')\n",
    "    plt2.plot(newy_test,color='blue', label='Actual')\n",
    "    plt2.legend(loc='best')\n",
    "    plt2.title('The test result for {}'.format(stock_name))\n",
    "    plt2.xlabel('Days')\n",
    "    plt2.ylabel('Adjusted Close')\n",
    "\n",
    "    figname = optimizer + stock_name + '_epochs' + str(epochs) + '_' + start.strftime(\"%Y%m%d\") + '-' + end.strftime(\"%Y%m%d\") + '.png'\n",
    "    plt2.savefig(figname, format='png', bbox_inches='tight', transparent=True)\n",
    "    plt2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_result(stock_name, p, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Save for consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('LSTM_Stock_prediction-20170429.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
